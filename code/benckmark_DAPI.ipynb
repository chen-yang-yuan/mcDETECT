{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0756a232",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyvips\n",
        "import tifffile\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy import ndimage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d81aee",
      "metadata": {},
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "990a340d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['mosaic_DAPI_z0.tif',\n",
              " 'mosaic_DAPI_z1.tif',\n",
              " 'mosaic_DAPI_z2.tif',\n",
              " 'mosaic_DAPI_z3.tif',\n",
              " 'mosaic_DAPI_z4.tif',\n",
              " 'mosaic_DAPI_z5.tif',\n",
              " 'mosaic_DAPI_z6.tif']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# File paths\n",
        "dataset = \"MERSCOPE_WT_1\"\n",
        "data_path = f\"../data/{dataset}/\"\n",
        "output_path = f\"../output/{dataset}/\"\n",
        "\n",
        "# Transformation parameters\n",
        "pixel_size = 0.10799861\n",
        "x_shift = int(-266.1734)\n",
        "y_shift = int(180.2510)\n",
        "\n",
        "# All DAPI images\n",
        "files = os.listdir(data_path + \"raw_data/DAPI_images/\")\n",
        "files = [i for i in files if i.startswith(\"mosaic\")]\n",
        "files.sort()\n",
        "files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d213bca",
      "metadata": {},
      "source": [
        "### Derive the MIP of DAPI images (run once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "83822b3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# paths = [data_path + \"raw_data/DAPI_images/\" + f\"mosaic_DAPI_z{i}.tif\" for i in range(7)]\n",
        "# imgs = [pyvips.Image.new_from_file(p, access=\"sequential\") for p in paths]\n",
        "# mip = imgs[0]\n",
        "# for im in imgs[1:]:\n",
        "#     mip = pyvips.Image.maxpair(mip, im)\n",
        "# mip.tiffsave(\n",
        "#     data_path + \"raw_data/DAPI_images/mosaic_DAPI_MIP.tif\",\n",
        "#     bigtiff=True,\n",
        "#     compression=\"lzw\",   # or \"deflate\"\n",
        "#     tile=True,\n",
        "#     tile_width=1024,\n",
        "#     tile_height=1024,\n",
        "#     pyramid=False\n",
        "# )\n",
        "# print(\"Saved:\", \"mosaic_DAPI_MIP.tif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393e1aeb",
      "metadata": {},
      "source": [
        "### Trial run: area of detected objects on the MIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0813b103",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== Helper functions ==================== #\n",
        "\n",
        "\n",
        "# Thresholding followed by dilation with circular kernel\n",
        "def adaptive_thresholding_with_dilation(img, block_size=49, c=-1, radius=10):\n",
        "    th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, c)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2*radius+1, 2*radius+1))\n",
        "    th_dilated = cv2.dilate(th, kernel)\n",
        "    return th_dilated\n",
        "\n",
        "\n",
        "def adaptive_thresholding_with_size_filter_and_dilation(img, block_size=49, c=-1, min_area=50, radius=10):\n",
        "    th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, c)\n",
        "    contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    filtered_contours = [c for c in contours if cv2.contourArea(c) >= min_area]\n",
        "    filtered_img = np.zeros_like(th)\n",
        "    cv2.drawContours(filtered_img, filtered_contours, -1, 255, -1)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2*radius+1, 2*radius+1))\n",
        "    th_dilated = cv2.dilate(filtered_img, kernel)\n",
        "    return th_dilated\n",
        "\n",
        "\n",
        "def otsu_with_dilation(img, radius=10):\n",
        "    _, th = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2*radius+1, 2*radius+1))\n",
        "    th_dilated = cv2.dilate(th, kernel)\n",
        "    return th_dilated\n",
        "\n",
        "\n",
        "# Analyze detected objects\n",
        "def analyze_objects(contours, method_name):\n",
        "    objects = []\n",
        "    for i, contour in enumerate(contours):\n",
        "        area_pixels = cv2.contourArea(contour)\n",
        "        if area_pixels > 0:\n",
        "            area_um2 = area_pixels * (pixel_size ** 2)\n",
        "            radius_um = np.sqrt(area_um2 / np.pi)\n",
        "            diameter_um = radius_um * 2\n",
        "            objects.append({\"method\": method_name,\n",
        "                            \"object_id\": i + 1,\n",
        "                            \"area_pixels\": area_pixels,\n",
        "                            \"area_um2\": area_um2,\n",
        "                            \"radius_um\": radius_um,\n",
        "                            \"diameter_um\": diameter_um})\n",
        "    return objects\n",
        "\n",
        "\n",
        "def print_method_results(objects, method_name):\n",
        "    \n",
        "    if len(objects) > 0:\n",
        "        \n",
        "        print(f\"Number of detected objects: {len(objects)}\")\n",
        "        print(f\"\\nDetailed statistics for each object:\")\n",
        "        \n",
        "        areas_px = [obj[\"area_pixels\"] for obj in objects]\n",
        "        areas_um2 = [obj[\"area_um2\"] for obj in objects]\n",
        "        radii_um = [obj[\"radius_um\"] for obj in objects]\n",
        "        diameters_um = [obj[\"diameter_um\"] for obj in objects]\n",
        "        \n",
        "        print(f\"\\nSummary statistics:\")\n",
        "        print(f\"  Area (pixels²):  Mean={np.mean(areas_px):6.1f}, Median={np.median(areas_px):6.1f}, Min={np.min(areas_px):6.1f}, Max={np.max(areas_px):6.1f}\")\n",
        "        print(f\"  Area (μm²):      Mean={np.mean(areas_um2):6.2f}, Median={np.median(areas_um2):6.2f}, Min={np.min(areas_um2):6.2f}, Max={np.max(areas_um2):6.2f}\")\n",
        "        print(f\"  Radius (μm):     Mean={np.mean(radii_um):6.2f}, Median={np.median(radii_um):6.2f}, Min={np.min(radii_um):6.2f}, Max={np.max(radii_um):6.2f}\")\n",
        "        print(f\"  Diameter (μm):   Mean={np.mean(diameters_um):6.2f}, Median={np.median(diameters_um):6.2f}, Min={np.min(diameters_um):6.2f}, Max={np.max(diameters_um):6.2f}\")\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        print(f\"No objects detected with {method_name}!\")\n",
        "\n",
        "\n",
        "# Extract statistics for plotting\n",
        "def get_method_statistics(method_results, method_name, stat_type=\"area_pixels\"):\n",
        "    if method_name not in method_results:\n",
        "        print(f\"Method '{method_name}' not found in results\")\n",
        "        return []\n",
        "    objects = method_results[method_name][\"objects\"]\n",
        "    return [obj[stat_type] for obj in objects]\n",
        "\n",
        "\n",
        "def get_all_methods_statistics(method_results, stat_type=\"area_pixels\"):\n",
        "    stats_dict = {}\n",
        "    for method_name in method_results.keys():\n",
        "        stats_dict[method_name] = get_method_statistics(method_results, method_name, stat_type)\n",
        "    return stats_dict\n",
        "\n",
        "\n",
        "# Downsample image\n",
        "def downsample_image(img, scale_factor=5000):\n",
        "    height, width = img.shape\n",
        "    if height < width:\n",
        "        scale_factor = 5000 / height\n",
        "        new_height = 5000\n",
        "        new_width = int(width * scale_factor)\n",
        "    else:\n",
        "        scale_factor = 5000 / width\n",
        "        new_width = 5000\n",
        "        new_height = int(height * scale_factor)\n",
        "    return cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d470d223",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize for comparison\n",
        "fname = \"mosaic_DAPI_MIP.tif\"\n",
        "\n",
        "# Load DAPI image, downsample, and save representative patch\n",
        "img_path = os.path.join(data_path, \"raw_data/DAPI_images\", fname)\n",
        "img = tifffile.imread(img_path)\n",
        "img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "img_downsampled = downsample_image(img, scale_factor=5000)\n",
        "cv2.imwrite(output_path + f\"downsampled_img_{fname}.png\", img_downsampled)\n",
        "\n",
        "target_coords = [(3750, 1500), (3750, 1650), (3750, 1800)]\n",
        "w, h = 150, 150\n",
        "for i, (x0, y0) in enumerate(target_coords):\n",
        "    patch = img_downsampled[y0:y0+h, x0:x0+w]\n",
        "    cv2.imwrite(output_path + f\"downsampled_patch_{i}_{fname}.png\", patch)\n",
        "\n",
        "print(f\"Analyzing DAPI image: {fname}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define thresholding methods\n",
        "thresholding_methods = {\n",
        "    \"Adaptive Thresholding\": {\n",
        "        \"function\": lambda img: cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1),\n",
        "        \"description\": \"Gaussian-weighted adaptive thresholding with block size 49\"\n",
        "    },\n",
        "    \"Adaptive + Dilation\": {\n",
        "        \"function\": lambda img: adaptive_thresholding_with_dilation(img, block_size=49, c=-1, radius=10),\n",
        "        \"description\": \"Adaptive thresholding followed by dilation with circular kernel (radius=10px)\"\n",
        "    },\n",
        "    \"Adaptive + Size Filter + Small Dilation\": {\n",
        "        \"function\": lambda img: adaptive_thresholding_with_size_filter_and_dilation(img, block_size=49, c=-1, min_area=100, radius=10),\n",
        "        \"description\": \"Adaptive thresholding followed by dilation with circular kernel (radius=10px) and size filter (min_area=100px)\"\n",
        "    },\n",
        "    \"Adaptive + Size Filter + Moderate Dilation\": {\n",
        "        \"function\": lambda img: adaptive_thresholding_with_size_filter_and_dilation(img, block_size=49, c=-1, min_area=100, radius=15),\n",
        "        \"description\": \"Adaptive thresholding followed by dilation with circular kernel (radius=15px) and size filter (min_area=100px)\"\n",
        "    },\n",
        "    \"Adaptive + Size Filter + Large Dilation\": {\n",
        "        \"function\": lambda img: adaptive_thresholding_with_size_filter_and_dilation(img, block_size=49, c=-1, min_area=100, radius=20),\n",
        "        \"description\": \"Adaptive thresholding followed by dilation with circular kernel (radius=20px) and size filter (min_area=100px)\"\n",
        "    },\n",
        "    \"Otsu Thresholding\": {\n",
        "        \"function\": lambda img: cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1],\n",
        "        \"description\": \"Automatic global thresholding using Otsu's method\"\n",
        "    },\n",
        "    \"Otsu + Dilation\": {\n",
        "        \"function\": lambda img: otsu_with_dilation(img, radius=10),\n",
        "        \"description\": \"Otsu thresholding followed by dilation with circular kernel (radius=10px)\"\n",
        "    },\n",
        "}\n",
        "\n",
        "print(f\"Total methods available: {list(thresholding_methods.keys())}\")\n",
        "\n",
        "# Main analysis loop\n",
        "all_results = {}\n",
        "\n",
        "for i, (method_name, method_info) in enumerate(thresholding_methods.items()):\n",
        "    print(f\"METHOD {i+1}: {method_name.upper()}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Description: {method_info['description']}\")\n",
        "    \n",
        "    # Apply thresholding method\n",
        "    thresholded_img = method_info[\"function\"](img)\n",
        "    \n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(thresholded_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    \n",
        "    # Save downsampled thresholded image\n",
        "    thresholded_img_downsampled = downsample_image(thresholded_img, scale_factor=5000)\n",
        "    safe_filename = method_name.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\").replace(\"+\", \"and\")\n",
        "    cv2.imwrite(output_path + f\"thresholded_img_{safe_filename}_downsampled.png\", thresholded_img_downsampled)\n",
        "    \n",
        "    for i, (x0, y0) in enumerate(target_coords):\n",
        "        patch = thresholded_img_downsampled[y0:y0+h, x0:x0+w]\n",
        "        cv2.imwrite(output_path + f\"thresholded_patch_{safe_filename}_downsampled_{i}.png\", patch)\n",
        "    \n",
        "    # Analyze objects\n",
        "    objects = analyze_objects(contours, method_name)\n",
        "    \n",
        "    # Print results\n",
        "    print_method_results(objects, method_name)\n",
        "    \n",
        "    # Store results\n",
        "    all_results[method_name] = {\n",
        "        \"objects\": objects,\n",
        "        \"thresholded_image\": thresholded_img,\n",
        "        \"num_objects\": len(objects)\n",
        "    }\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Method Comparison Summary\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"-\" * 40)\n",
        "for method_name, results in all_results.items():\n",
        "    print(f\"{method_name}: {results['num_objects']} objects detected\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Store results for further analysis\n",
        "method_results = all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c3d98c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage - extract areas and radii for each method\n",
        "print(\"EXTRACTING STATISTICS FOR PLOTTING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Get area statistics (pixels) for each method\n",
        "areas_pixels = get_all_methods_statistics(method_results, \"area_pixels\")\n",
        "print(\"Areas (pixels²) for each method:\")\n",
        "for method, areas in areas_pixels.items():\n",
        "    print(f\"  {method}: {len(areas)} objects, areas = {areas[:5]}...\" if len(areas) > 5 else f\"  {method}: {len(areas)} objects, areas = {areas}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Get area statistics (μm²) for each method  \n",
        "areas_um2 = get_all_methods_statistics(method_results, \"area_um2\")\n",
        "print(\"Areas (μm²) for each method:\")\n",
        "for method, areas in areas_um2.items():\n",
        "    print(f\"  {method}: {len(areas)} objects, areas = {[f'{a:.2f}' for a in areas[:5]]}...\" if len(areas) > 5 else f\"  {method}: {len(areas)} objects, areas = {[f'{a:.2f}' for a in areas]}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Get radius statistics (μm) for each method\n",
        "radii_um = get_all_methods_statistics(method_results, \"radius_um\")\n",
        "print(\"Radii (μm) for each method:\")\n",
        "for method, radii in radii_um.items():\n",
        "    print(f\"  {method}: {len(radii)} objects, radii = {[f'{r:.2f}' for r in radii[:5]]}...\" if len(radii) > 5 else f\"  {method}: {len(radii)} objects, radii = {[f'{r:.2f}' for r in radii]}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Get diameter statistics (μm) for each method\n",
        "diameters_um = get_all_methods_statistics(method_results, \"diameter_um\")\n",
        "print(\"Diameters (μm) for each method:\")\n",
        "for method, diameters in diameters_um.items():\n",
        "    print(f\"  {method}: {len(diameters)} objects, diameters = {[f'{d:.2f}' for d in diameters[:5]]}...\" if len(diameters) > 5 else f\"  {method}: {len(diameters)} objects, diameters = {[f'{d:.2f}' for d in diameters]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Variables available for plotting:\")\n",
        "print(\"- areas_pixels: dict with areas in pixels² for each method\")\n",
        "print(\"- areas_um2: dict with areas in μm² for each method\") \n",
        "print(\"- radii_um: dict with radii in μm for each method\")\n",
        "print(\"- diameters_um: dict with diameters in μm for each method\")\n",
        "print(\"\\nExample access:\")\n",
        "print(\"- areas_pixels['Adaptive Thresholding'] -> list of areas\")\n",
        "print(\"- radii_um['Otsu Thresholding'] -> list of radii\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f2841f",
      "metadata": {},
      "outputs": [],
      "source": [
        "areas_um2.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ad8f230",
      "metadata": {},
      "outputs": [],
      "source": [
        "aaa = areas_um2[\"Adaptive + Size Filter + Dilation 2\"]\n",
        "np.mean(aaa), np.median(aaa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a7c831",
      "metadata": {},
      "outputs": [],
      "source": [
        "cutoff1 = np.percentile(aaa, 1) \n",
        "cutoff2 = np.percentile(aaa, 99.5)\n",
        "bbb = [i for i in aaa if i > cutoff1 and i < cutoff2]\n",
        "np.mean(bbb), np.median(bbb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a841a7a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "cutoff1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e27140",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "ccc = [i for i in bbb if i > 0]\n",
        "med = np.median(ccc)\n",
        "plt.figure(figsize = (15, 4))\n",
        "sns.histplot(ccc, binwidth=5, kde=False, edgecolor=\"gray\")\n",
        "if np.isfinite(med):\n",
        "    plt.axvline(med, color=\"red\", linestyle=\"--\", linewidth=1)\n",
        "    ymax = plt.ylim()[1]\n",
        "    plt.text(med + 20, ymax * 0.95, f\"{med:.2f}\",\n",
        "            color=\"red\", ha=\"left\", va=\"top\", fontsize=10)\n",
        "plt.xlabel(\"Soma area\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4612d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example plotting code for area/radius distributions\n",
        "\n",
        "# Plot area distributions (μm²)\n",
        "if any(len(areas) > 0 for areas in areas_um2.values()):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Histogram comparison\n",
        "    ax1.set_title(\"Area Distribution Comparison\")\n",
        "    ax1.set_xlabel(\"Area (μm²)\")\n",
        "    ax1.set_ylabel(\"Frequency\")\n",
        "    \n",
        "    colors = [\"blue\", \"orange\", \"green\", \"red\", \"purple\"]\n",
        "    for i, (method, areas) in enumerate(areas_um2.items()):\n",
        "        if len(areas) > 0:\n",
        "            ax1.hist(areas, alpha=0.6, label=f\"{method} (n={len(areas)})\", \n",
        "                    color=colors[i % len(colors)], bins=20)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Box plot comparison\n",
        "    ax2.set_title(\"Area Distribution Box Plot\")\n",
        "    ax2.set_ylabel(\"Area (μm²)\")\n",
        "    \n",
        "    method_names = []\n",
        "    area_data = []\n",
        "    for method, areas in areas_um2.items():\n",
        "        if len(areas) > 0:\n",
        "            method_names.append(method.replace(\" Thresholding\", \"\"))  # Shorter labels\n",
        "            area_data.append(areas)\n",
        "    \n",
        "    if area_data:\n",
        "        ax2.boxplot(area_data, labels=method_names)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        plt.setp(ax2.get_xticklabels(), rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot radius distributions (μm)\n",
        "if any(len(radii) > 0 for radii in radii_um.values()):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Histogram comparison\n",
        "    ax1.set_title(\"Radius Distribution Comparison\")\n",
        "    ax1.set_xlabel(\"Radius (μm)\")\n",
        "    ax1.set_ylabel(\"Frequency\")\n",
        "    \n",
        "    for i, (method, radii) in enumerate(radii_um.items()):\n",
        "        if len(radii) > 0:\n",
        "            ax1.hist(radii, alpha=0.6, label=f\"{method} (n={len(radii)})\", \n",
        "                    color=colors[i % len(colors)], bins=20)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Box plot comparison\n",
        "    ax2.set_title(\"Radius Distribution Box Plot\")\n",
        "    ax2.set_ylabel(\"Radius (μm)\")\n",
        "    \n",
        "    method_names = []\n",
        "    radius_data = []\n",
        "    for method, radii in radii_um.items():\n",
        "        if len(radii) > 0:\n",
        "            method_names.append(method.replace(\" Thresholding\", \"\"))\n",
        "            radius_data.append(radii)\n",
        "    \n",
        "    if radius_data:\n",
        "        ax2.boxplot(radius_data, labels=method_names)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        plt.setp(ax2.get_xticklabels(), rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Plots generated! You can modify the plotting code above to customize the visualizations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1575d9b0",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d2594ad6",
      "metadata": {},
      "source": [
        "### Transcripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99fcbdc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read transcripts\n",
        "transcripts = pd.read_csv(data_path + \"raw_data/transcripts.csv\")\n",
        "transcripts = transcripts[[\"cell_id\", \"gene\", \"global_x\", \"global_y\", \"global_z\"]].copy()\n",
        "transcripts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1b8036",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define target genes\n",
        "all_genes = pd.read_csv(data_path + \"processed_data/genes.csv\")\n",
        "all_genes = all_genes[\"genes\"].tolist()\n",
        "\n",
        "granule_markers = [\"Camk2a\", \"Cplx2\", \"Slc17a7\", \"Ddn\", \"Syp\", \"Map1a\", \"Shank1\", \"Syn1\", \"Gria1\", \"Gria2\", \"Cyfip2\", \"Vamp2\", \"Bsn\", \"Slc32a1\", \"Nfasc\", \"Syt1\", \"Tubb3\", \"Nav1\", \"Shank3\", \"Mapt\"]\n",
        "\n",
        "nc_markers = pd.read_csv(data_path + \"processed_data/negative_controls.csv\")\n",
        "nc_markers = nc_markers[\"Gene\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2039dd27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Main operations on transcripts\n",
        "\n",
        "# # Compute DAPI pixel coordinates\n",
        "# transcripts[\"row\"] = (transcripts[\"global_y\"] / pixel_size).astype(int) + y_shift\n",
        "# transcripts[\"col\"] = (transcripts[\"global_x\"] / pixel_size).astype(int) + x_shift\n",
        "\n",
        "# # Add default overlap column\n",
        "# transcripts[\"overlaps_nucleus\"] = 0\n",
        "\n",
        "# # Update labels in place\n",
        "# global_ratio = []\n",
        "\n",
        "# for j, fname in enumerate(files):\n",
        "    \n",
        "#     # Load DAPI image\n",
        "#     img = tifffile.imread(f\"raw_data/DAPI_images/{fname}\")\n",
        "#     img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "    \n",
        "#     # Threshold and dilate\n",
        "#     th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1)\n",
        "#     th = cv2.dilate(th, np.ones((3, 3), np.uint8), iterations=2)\n",
        "\n",
        "#     # Save resized visualization (optional)\n",
        "#     th_small = cv2.resize(th, (3500, 5000), interpolation=cv2.INTER_AREA)\n",
        "#     cv2.imwrite(f\"intermediate_data/images/z_{j}_small.png\", th_small)\n",
        "    \n",
        "#     # Select transcripts in this z-layer\n",
        "#     trans_z_mask = transcripts[\"global_z\"] == j\n",
        "#     trans_z = transcripts[trans_z_mask].copy()\n",
        "#     row_vals = trans_z[\"row\"].astype(int).values\n",
        "#     col_vals = trans_z[\"col\"].astype(int).values\n",
        "\n",
        "#     # Avoid out-of-bounds indexing\n",
        "#     height, width = th.shape\n",
        "#     valid = (row_vals >= 0) & (row_vals < height) & (col_vals >= 0) & (col_vals < width)\n",
        "#     row_valid = row_vals[valid]\n",
        "#     col_valid = col_vals[valid]\n",
        "    \n",
        "#     # Assign in-nucleus labels\n",
        "#     overlaps = np.zeros(len(trans_z), dtype=int)\n",
        "#     overlaps[valid] = (th[row_valid, col_valid] != 0).astype(int)\n",
        "\n",
        "#     # Update main DataFrame in-place\n",
        "#     transcripts.loc[trans_z.index, \"overlaps_nucleus\"] = overlaps\n",
        "\n",
        "#     # Track global ratio\n",
        "#     global_ratio.append(overlaps.mean())\n",
        "#     print(f\"Iteration {j+1}: {np.sum(row_vals != row_valid)} row mismatches, {np.sum(col_vals != col_valid)} column mismatches, {overlaps.mean():.2%} in-nucleus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd31d6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Final labeled transcripts\n",
        "# transcripts = transcripts[[\"cell_id\", \"overlaps_nucleus\", \"gene\", \"global_x\", \"global_y\", \"global_z\"]].copy()\n",
        "# transcripts[\"global_z\"] *= 1.5\n",
        "# transcripts = transcripts.rename(columns = {\"gene\": \"target\"})\n",
        "# transcripts.to_parquet(\"intermediate_data/transcripts.parquet\")\n",
        "# transcripts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99829ce8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== Initial Nuclei Mask Analysis (No Dilation) ====================#\n",
        "# MEMORY-EFFICIENT VERSION - Copy this into your notebook cell\n",
        "\n",
        "import gc\n",
        "\n",
        "# Read all DAPI images from different z-layers (memory-efficient incremental processing)\n",
        "print(f\"Reading all DAPI images from {len(files)} z-layers...\\n\")\n",
        "\n",
        "# First pass: get image dimensions\n",
        "print(\"First pass: Getting image dimensions...\")\n",
        "first_img_path = os.path.join(data_path, \"raw_data/DAPI_images\", files[0])\n",
        "first_img = tifffile.imread(first_img_path)\n",
        "first_img = cv2.normalize(first_img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "height, width = first_img.shape\n",
        "print(f\"Image shape: {height} x {width}\\n\")\n",
        "\n",
        "# Initialize accumulators for memory-efficient processing\n",
        "non_zero_count = np.zeros((height, width), dtype=np.uint8)\n",
        "mip_accumulator = first_img.copy().astype(np.float32)\n",
        "mean_accumulator = first_img.copy().astype(np.float32)\n",
        "median_list = [first_img.astype(np.float32)]\n",
        "\n",
        "# Process first image\n",
        "th_first = cv2.adaptiveThreshold(first_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1)\n",
        "non_zero_count += (th_first > 0).astype(np.uint8)\n",
        "\n",
        "# Process remaining images incrementally (one at a time to save memory)\n",
        "print(\"Processing images incrementally to save memory...\")\n",
        "for i, fname in enumerate(files[1:], start=1):\n",
        "    print(f\"  Processing layer {i+1}/{len(files)}: {fname}\")\n",
        "    img_path = os.path.join(data_path, \"raw_data/DAPI_images\", fname)\n",
        "    img = tifffile.imread(img_path)\n",
        "    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "    \n",
        "    # Update accumulators\n",
        "    th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1)\n",
        "    non_zero_count += (th > 0).astype(np.uint8)\n",
        "    mip_accumulator = np.maximum(mip_accumulator, img.astype(np.float32))\n",
        "    mean_accumulator += img.astype(np.float32)\n",
        "    median_list.append(img.astype(np.float32))\n",
        "    \n",
        "    # Free memory immediately\n",
        "    del img, th\n",
        "    if i % 2 == 0:  # Force garbage collection every 2 images\n",
        "        gc.collect()\n",
        "\n",
        "# Finalize accumulators\n",
        "mean_accumulator /= len(files)\n",
        "mip_accumulator = mip_accumulator.astype(np.uint8)\n",
        "mean_accumulator = mean_accumulator.astype(np.uint8)\n",
        "\n",
        "# Compute median (still requires stacking, but only for median)\n",
        "print(\"Computing median projection...\")\n",
        "median_accumulator = np.median(np.stack(median_list, axis=0), axis=0).astype(np.uint8)\n",
        "del median_list, first_img, th_first\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Completed processing all {len(files)} images\\n\")\n",
        "num_layers = len(files)\n",
        "\n",
        "# Define stacking strategies (now using pre-computed accumulators)\n",
        "strategies = {\n",
        "    \"At least 1 layer\": {\n",
        "        \"mask\": lambda: (non_zero_count >= 1).astype(np.uint8) * 255,\n",
        "        \"description\": \"Pixel is 1 if non-zero in at least 1 layer\"\n",
        "    },\n",
        "    \"At least 2 layers\": {\n",
        "        \"mask\": lambda: (non_zero_count >= 2).astype(np.uint8) * 255,\n",
        "        \"description\": \"Pixel is 1 if non-zero in at least 2 layers\"\n",
        "    },\n",
        "    \"At least 3 layers\": {\n",
        "        \"mask\": lambda: (non_zero_count >= 3).astype(np.uint8) * 255,\n",
        "        \"description\": \"Pixel is 1 if non-zero in at least 3 layers\"\n",
        "    },\n",
        "    \"At least 4 layers\": {\n",
        "        \"mask\": lambda: (non_zero_count >= 4).astype(np.uint8) * 255,\n",
        "        \"description\": \"Pixel is 1 if non-zero in at least 4 layers\"\n",
        "    },\n",
        "    \"Maximum Intensity Projection (MIP)\": {\n",
        "        \"mask\": lambda: cv2.adaptiveThreshold(\n",
        "            mip_accumulator,\n",
        "            255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1\n",
        "        ),\n",
        "        \"description\": \"Take max across layers, then threshold\"\n",
        "    },\n",
        "    \"Mean Intensity Projection\": {\n",
        "        \"mask\": lambda: cv2.adaptiveThreshold(\n",
        "            mean_accumulator,\n",
        "            255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1\n",
        "        ),\n",
        "        \"description\": \"Take mean across layers, then threshold\"\n",
        "    },\n",
        "    \"Median Intensity Projection\": {\n",
        "        \"mask\": lambda: cv2.adaptiveThreshold(\n",
        "            median_accumulator,\n",
        "            255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1\n",
        "        ),\n",
        "        \"description\": \"Take median across layers, then threshold\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Calculate downsampling dimensions (once, since all images have same size)\n",
        "if height < width:\n",
        "    scale_factor = 5000 / height\n",
        "    new_height = 5000\n",
        "    new_width = int(width * scale_factor)\n",
        "else:\n",
        "    scale_factor = 5000 / width\n",
        "    new_width = 5000\n",
        "    new_height = int(height * scale_factor)\n",
        "\n",
        "# Create output directory\n",
        "output_dir = os.path.join(data_path, \"intermediate_data\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process each strategy\n",
        "results = []\n",
        "for strategy_name, strategy_info in strategies.items():\n",
        "    print(\"=\"*80)\n",
        "    print(f\"STRATEGY: {strategy_name}\")\n",
        "    print(f\"Description: {strategy_info[\"description\"]}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Generate merged mask\n",
        "    merged_mask = strategy_info[\"mask\"]()\n",
        "    \n",
        "    # Detect individual nuclei masks using findContours\n",
        "    contours, _ = cv2.findContours(merged_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    \n",
        "    # Calculate areas for each detected nucleus\n",
        "    nuclei_areas_pixels = []\n",
        "    nuclei_areas_um2 = []\n",
        "    nuclei_radii_um = []\n",
        "    \n",
        "    for contour in contours:\n",
        "        area_pixels = cv2.contourArea(contour)\n",
        "        if area_pixels > 0:  # Filter out tiny artifacts\n",
        "            nuclei_areas_pixels.append(area_pixels)\n",
        "            area_um2 = area_pixels * (pixel_size ** 2)\n",
        "            nuclei_areas_um2.append(area_um2)\n",
        "            # Approximate radius assuming circular shape: area = π * r^2\n",
        "            radius_um = np.sqrt(area_um2 / np.pi)\n",
        "            nuclei_radii_um.append(radius_um)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    if len(nuclei_areas_pixels) > 0:\n",
        "        print(f\"Number of detected nuclei: {len(nuclei_areas_pixels)}\")\n",
        "        print(f\"\\nArea statistics (pixels²):\")\n",
        "        print(f\"  Mean: {np.mean(nuclei_areas_pixels):.1f}\")\n",
        "        print(f\"  Median: {np.median(nuclei_areas_pixels):.1f}\")\n",
        "        print(f\"  Min: {np.min(nuclei_areas_pixels):.1f}\")\n",
        "        print(f\"  Max: {np.max(nuclei_areas_pixels):.1f}\")\n",
        "        print(f\"  Std: {np.std(nuclei_areas_pixels):.1f}\")\n",
        "        \n",
        "        print(f\"\\nArea statistics (μm²):\")\n",
        "        print(f\"  Mean: {np.mean(nuclei_areas_um2):.2f}\")\n",
        "        print(f\"  Median: {np.median(nuclei_areas_um2):.2f}\")\n",
        "        print(f\"  Min: {np.min(nuclei_areas_um2):.2f}\")\n",
        "        print(f\"  Max: {np.max(nuclei_areas_um2):.2f}\")\n",
        "        print(f\"  Std: {np.std(nuclei_areas_um2):.2f}\")\n",
        "        \n",
        "        print(f\"\\nRadius statistics (μm, approximated as circle):\")\n",
        "        print(f\"  Mean: {np.mean(nuclei_radii_um):.2f}\")\n",
        "        print(f\"  Median: {np.median(nuclei_radii_um):.2f}\")\n",
        "        print(f\"  Min: {np.min(nuclei_radii_um):.2f}\")\n",
        "        print(f\"  Max: {np.max(nuclei_radii_um):.2f}\")\n",
        "        print(f\"  Std: {np.std(nuclei_radii_um):.2f}\")\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            \"strategy\": strategy_name,\n",
        "            \"num_nuclei\": len(nuclei_areas_pixels),\n",
        "            \"mean_area_pixels\": np.mean(nuclei_areas_pixels),\n",
        "            \"median_area_pixels\": np.median(nuclei_areas_pixels),\n",
        "            \"mean_area_um2\": np.mean(nuclei_areas_um2),\n",
        "            \"median_area_um2\": np.median(nuclei_areas_um2),\n",
        "            \"mean_radius_um\": np.mean(nuclei_radii_um),\n",
        "            \"median_radius_um\": np.median(nuclei_radii_um)\n",
        "        })\n",
        "    else:\n",
        "        print(\"No nuclei detected!\")\n",
        "        results.append({\n",
        "            \"strategy\": strategy_name,\n",
        "            \"num_nuclei\": 0,\n",
        "            \"mean_area_pixels\": 0,\n",
        "            \"median_area_pixels\": 0,\n",
        "            \"mean_area_um2\": 0,\n",
        "            \"median_area_um2\": 0,\n",
        "            \"mean_radius_um\": 0,\n",
        "            \"median_radius_um\": 0\n",
        "        })\n",
        "    \n",
        "    # Save downsampled merged mask\n",
        "    merged_mask_downsampled = cv2.resize(merged_mask, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "    safe_filename = strategy_name.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\")\n",
        "    output_path = os.path.join(output_dir, f\"merged_mask_{safe_filename}_downsampled.png\")\n",
        "    cv2.imwrite(output_path, merged_mask_downsampled)\n",
        "    print(f\"\\nDownsampled merged mask saved to: {output_path}\")\n",
        "    print(f\"Downsampled size: {new_height} x {new_width} pixels\\n\")\n",
        "    \n",
        "    # Free memory after processing each strategy\n",
        "    del merged_mask, merged_mask_downsampled, contours\n",
        "    gc.collect()\n",
        "\n",
        "# Clean up large accumulators\n",
        "del non_zero_count, mip_accumulator, mean_accumulator, median_accumulator\n",
        "gc.collect()\n",
        "\n",
        "# Print summary table\n",
        "print(\"=\"*80)\n",
        "print(\"SUMMARY OF ALL STRATEGIES\")\n",
        "print(\"=\"*80)\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_path = os.path.join(output_dir, \"stacking_strategies_summary.csv\")\n",
        "results_df.to_csv(summary_path, index=False)\n",
        "print(f\"Summary saved to: {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5050ff38",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.median([i for i in nuclei_areas_pixels if i > 1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30aa53e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== Dilation Benchmarking ====================#\n",
        "\n",
        "# Benchmark settings\n",
        "kernel_sizes = [3, 5]\n",
        "iterations_list = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "print(\"Starting dilation benchmarking...\\n\")\n",
        "\n",
        "for kernel_size in kernel_sizes:\n",
        "    for iterations in iterations_list:\n",
        "        print(f\"Testing: {kernel_size}x{kernel_size} kernel, {iterations} iterations\")\n",
        "        \n",
        "        # Initialize counters for this setting\n",
        "        all_areas = []  # Store areas of all detected nuclei masks\n",
        "        all_transcripts_overlap = []  # Track overlap for all transcripts\n",
        "        granule_overlap = []  # Track overlap for granule markers\n",
        "        nc_overlap = []  # Track overlap for negative controls\n",
        "        \n",
        "        # Process each z-layer\n",
        "        for j, fname in enumerate(files):\n",
        "            # Load DAPI image\n",
        "            img_path = os.path.join(data_path, \"raw_data/DAPI_images\", fname)\n",
        "            img = tifffile.imread(img_path)\n",
        "            img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "            \n",
        "            # Threshold and dilate with current settings\n",
        "            th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1)\n",
        "            kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "            th = cv2.dilate(th, kernel, iterations=iterations)\n",
        "            \n",
        "            # Detect individual nuclei masks using findContours\n",
        "            contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            \n",
        "            # Calculate areas for each detected nucleus\n",
        "            for contour in contours:\n",
        "                area_pixels = cv2.contourArea(contour)\n",
        "                if area_pixels > 0:  # Filter out tiny artifacts\n",
        "                    all_areas.append(area_pixels)\n",
        "            \n",
        "            # Map transcripts to this z-layer and check overlap\n",
        "            trans_z_mask = transcripts[\"global_z\"] == j\n",
        "            trans_z = transcripts[trans_z_mask].copy()\n",
        "            row_vals = trans_z[\"row\"].astype(int).values\n",
        "            col_vals = trans_z[\"col\"].astype(int).values\n",
        "            \n",
        "            # Avoid out-of-bounds indexing\n",
        "            height, width = th.shape\n",
        "            valid = (row_vals >= 0) & (row_vals < height) & (col_vals >= 0) & (col_vals < width)\n",
        "            row_valid = row_vals[valid]\n",
        "            col_valid = col_vals[valid]\n",
        "            \n",
        "            # Check overlap for valid transcripts\n",
        "            overlaps = np.zeros(len(trans_z), dtype=int)\n",
        "            overlaps[valid] = (th[row_valid, col_valid] != 0).astype(int)\n",
        "            \n",
        "            # Store overlap information\n",
        "            for idx, overlap in enumerate(overlaps):\n",
        "                gene = trans_z.iloc[idx][\"gene\"]\n",
        "                all_transcripts_overlap.append(overlap)\n",
        "                \n",
        "                if gene in granule_markers:\n",
        "                    granule_overlap.append(overlap)\n",
        "                if gene in nc_markers:\n",
        "                    nc_overlap.append(overlap)\n",
        "        \n",
        "        # Calculate statistics\n",
        "        if len(all_areas) > 0:\n",
        "            avg_area_pixels = np.mean(all_areas)\n",
        "            avg_area_um2 = avg_area_pixels * (pixel_size ** 2)\n",
        "            # Approximate radius assuming circular shape: area = π * r^2\n",
        "            avg_radius_um = np.sqrt(avg_area_um2 / np.pi)\n",
        "        else:\n",
        "            avg_area_pixels = 0\n",
        "            avg_area_um2 = 0\n",
        "            avg_radius_um = 0\n",
        "        \n",
        "        # Calculate extrasomatic fractions (1 - overlap fraction)\n",
        "        if len(all_transcripts_overlap) > 0:\n",
        "            extrasomatic_all = 1 - np.mean(all_transcripts_overlap)\n",
        "        else:\n",
        "            extrasomatic_all = 0\n",
        "        \n",
        "        if len(granule_overlap) > 0:\n",
        "            extrasomatic_granule = 1 - np.mean(granule_overlap)\n",
        "        else:\n",
        "            extrasomatic_granule = 0\n",
        "        \n",
        "        if len(nc_overlap) > 0:\n",
        "            extrasomatic_nc = 1 - np.mean(nc_overlap)\n",
        "        else:\n",
        "            extrasomatic_nc = 0\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            \"kernel_size\": kernel_size,\n",
        "            \"iterations\": iterations,\n",
        "            \"avg_area_pixels\": avg_area_pixels,\n",
        "            \"avg_area_um2\": avg_area_um2,\n",
        "            \"avg_radius_um\": avg_radius_um,\n",
        "            \"extrasomatic_all_genes\": extrasomatic_all,\n",
        "            \"extrasomatic_granule_markers\": extrasomatic_granule,\n",
        "            \"extrasomatic_negative_controls\": extrasomatic_nc,\n",
        "            \"num_nuclei\": len(all_areas),\n",
        "            \"num_transcripts_all\": len(all_transcripts_overlap),\n",
        "            \"num_transcripts_granule\": len(granule_overlap),\n",
        "            \"num_transcripts_nc\": len(nc_overlap)\n",
        "        })\n",
        "        \n",
        "        print(f\"  Average area: {avg_area_pixels:.1f} pixels² ({avg_area_um2:.2f} μm²), radius: {avg_radius_um:.2f} μm\")\n",
        "        print(f\"  Extrasomatic fractions: All={extrasomatic_all:.2%}, Granule={extrasomatic_granule:.2%}, NC={extrasomatic_nc:.2%}\\n\")\n",
        "\n",
        "# Convert results to DataFrame and display\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BENCHMARK RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv(os.path.join(data_path, \"intermediate_data\", \"dilation_benchmark_results.csv\"), index=False)\n",
        "print(f\"\\nResults saved to: {os.path.join(data_path, \"intermediate_data\", \"dilation_benchmark_results.csv\")}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mcDETECT-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
