{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0756a232",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tifffile\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "990a340d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== Settings ====================#\n",
        "\n",
        "# File paths\n",
        "dataset = \"MERSCOPE_WT_1\"\n",
        "data_path = f\"../data/{dataset}/\"\n",
        "output_path = f\"../output/{dataset}/\"\n",
        "\n",
        "# Transformation parameters\n",
        "pixel_size = 0.10799861\n",
        "x_shift = int(-266.1734)\n",
        "y_shift = int(180.2510)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "04e5f8c8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['mosaic_DAPI_z0.tif',\n",
              " 'mosaic_DAPI_z1.tif',\n",
              " 'mosaic_DAPI_z2.tif',\n",
              " 'mosaic_DAPI_z3.tif',\n",
              " 'mosaic_DAPI_z4.tif',\n",
              " 'mosaic_DAPI_z5.tif',\n",
              " 'mosaic_DAPI_z6.tif']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# All DAPI images\n",
        "files = os.listdir(data_path + \"raw_data/DAPI_images/\")\n",
        "files = [i for i in files if i.startswith(\"mosaic\")]\n",
        "files.sort()\n",
        "files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99fcbdc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read transcripts\n",
        "transcripts = pd.read_csv(data_path + \"raw_data/transcripts.csv\")\n",
        "transcripts = transcripts[[\"cell_id\", \"gene\", \"global_x\", \"global_y\", \"global_z\"]].copy()\n",
        "transcripts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1b8036",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define target genes\n",
        "all_genes = pd.read_csv(data_path + \"processed_data/genes.csv\")\n",
        "all_genes = all_genes[\"genes\"].tolist()\n",
        "\n",
        "granule_markers = [\"Camk2a\", \"Cplx2\", \"Slc17a7\", \"Ddn\", \"Syp\", \"Map1a\", \"Shank1\", \"Syn1\", \"Gria1\", \"Gria2\", \"Cyfip2\", \"Vamp2\", \"Bsn\", \"Slc32a1\", \"Nfasc\", \"Syt1\", \"Tubb3\", \"Nav1\", \"Shank3\", \"Mapt\"]\n",
        "\n",
        "nc_markers = pd.read_csv(data_path + \"processed_data/negative_controls.csv\")\n",
        "nc_markers = nc_markers[\"Gene\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2039dd27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Main operations on transcripts\n",
        "\n",
        "# # Compute DAPI pixel coordinates\n",
        "# transcripts[\"row\"] = (transcripts[\"global_y\"] / pixel_size).astype(int) + y_shift\n",
        "# transcripts[\"col\"] = (transcripts[\"global_x\"] / pixel_size).astype(int) + x_shift\n",
        "\n",
        "# # Add default overlap column\n",
        "# transcripts[\"overlaps_nucleus\"] = 0\n",
        "\n",
        "# # Update labels in place\n",
        "# global_ratio = []\n",
        "\n",
        "# for j, fname in enumerate(files):\n",
        "    \n",
        "#     # Load DAPI image\n",
        "#     img = tifffile.imread(f\"raw_data/DAPI_images/{fname}\")\n",
        "#     img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "    \n",
        "#     # Threshold and dilate\n",
        "#     th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1)\n",
        "#     th = cv2.dilate(th, np.ones((3, 3), np.uint8), iterations=2)\n",
        "\n",
        "#     # Save resized visualization (optional)\n",
        "#     th_small = cv2.resize(th, (3500, 5000), interpolation=cv2.INTER_AREA)\n",
        "#     cv2.imwrite(f\"intermediate_data/images/z_{j}_small.png\", th_small)\n",
        "    \n",
        "#     # Select transcripts in this z-layer\n",
        "#     trans_z_mask = transcripts[\"global_z\"] == j\n",
        "#     trans_z = transcripts[trans_z_mask].copy()\n",
        "#     row_vals = trans_z[\"row\"].astype(int).values\n",
        "#     col_vals = trans_z[\"col\"].astype(int).values\n",
        "\n",
        "#     # Avoid out-of-bounds indexing\n",
        "#     height, width = th.shape\n",
        "#     valid = (row_vals >= 0) & (row_vals < height) & (col_vals >= 0) & (col_vals < width)\n",
        "#     row_valid = row_vals[valid]\n",
        "#     col_valid = col_vals[valid]\n",
        "    \n",
        "#     # Assign in-nucleus labels\n",
        "#     overlaps = np.zeros(len(trans_z), dtype=int)\n",
        "#     overlaps[valid] = (th[row_valid, col_valid] != 0).astype(int)\n",
        "\n",
        "#     # Update main DataFrame in-place\n",
        "#     transcripts.loc[trans_z.index, \"overlaps_nucleus\"] = overlaps\n",
        "\n",
        "#     # Track global ratio\n",
        "#     global_ratio.append(overlaps.mean())\n",
        "#     print(f\"Iteration {j+1}: {np.sum(row_vals != row_valid)} row mismatches, {np.sum(col_vals != col_valid)} column mismatches, {overlaps.mean():.2%} in-nucleus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd31d6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Final labeled transcripts\n",
        "# transcripts = transcripts[[\"cell_id\", \"overlaps_nucleus\", \"gene\", \"global_x\", \"global_y\", \"global_z\"]].copy()\n",
        "# transcripts[\"global_z\"] *= 1.5\n",
        "# transcripts = transcripts.rename(columns = {\"gene\": \"target\"})\n",
        "# transcripts.to_parquet(\"intermediate_data/transcripts.parquet\")\n",
        "# transcripts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "99829ce8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading all DAPI images from 7 z-layers...\n",
            "\n",
            "First pass: Getting image dimensions...\n",
            "Image shape: 104161 x 70528\n",
            "\n",
            "Processing images incrementally to save memory...\n",
            "  Processing layer 2/7: mosaic_DAPI_z1.tif\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# ==================== Initial Nuclei Mask Analysis (No Dilation) ====================#\n",
        "# MEMORY-EFFICIENT VERSION - Copy this into your notebook cell\n",
        "\n",
        "import gc\n",
        "\n",
        "# Read all DAPI images from different z-layers (memory-efficient incremental processing)\n",
        "print(f\"Reading all DAPI images from {len(files)} z-layers...\\n\")\n",
        "\n",
        "# First pass: get image dimensions\n",
        "print(\"First pass: Getting image dimensions...\")\n",
        "first_img_path = os.path.join(data_path, \"raw_data/DAPI_images\", files[0])\n",
        "first_img = tifffile.imread(first_img_path)\n",
        "first_img = cv2.normalize(first_img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "height, width = first_img.shape\n",
        "print(f\"Image shape: {height} x {width}\\n\")\n",
        "\n",
        "# Initialize accumulators for memory-efficient processing\n",
        "non_zero_count = np.zeros((height, width), dtype=np.uint8)\n",
        "mip_accumulator = first_img.copy().astype(np.float32)\n",
        "mean_accumulator = first_img.copy().astype(np.float32)\n",
        "median_list = [first_img.astype(np.float32)]\n",
        "\n",
        "# Process first image\n",
        "th_first = cv2.adaptiveThreshold(first_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1)\n",
        "non_zero_count += (th_first > 0).astype(np.uint8)\n",
        "\n",
        "# Process remaining images incrementally (one at a time to save memory)\n",
        "print(\"Processing images incrementally to save memory...\")\n",
        "for i, fname in enumerate(files[1:], start=1):\n",
        "    print(f\"  Processing layer {i+1}/{len(files)}: {fname}\")\n",
        "    img_path = os.path.join(data_path, \"raw_data/DAPI_images\", fname)\n",
        "    img = tifffile.imread(img_path)\n",
        "    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "    \n",
        "    # Update accumulators\n",
        "    th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1)\n",
        "    non_zero_count += (th > 0).astype(np.uint8)\n",
        "    mip_accumulator = np.maximum(mip_accumulator, img.astype(np.float32))\n",
        "    mean_accumulator += img.astype(np.float32)\n",
        "    median_list.append(img.astype(np.float32))\n",
        "    \n",
        "    # Free memory immediately\n",
        "    del img, th\n",
        "    if i % 2 == 0:  # Force garbage collection every 2 images\n",
        "        gc.collect()\n",
        "\n",
        "# Finalize accumulators\n",
        "mean_accumulator /= len(files)\n",
        "mip_accumulator = mip_accumulator.astype(np.uint8)\n",
        "mean_accumulator = mean_accumulator.astype(np.uint8)\n",
        "\n",
        "# Compute median (still requires stacking, but only for median)\n",
        "print(\"Computing median projection...\")\n",
        "median_accumulator = np.median(np.stack(median_list, axis=0), axis=0).astype(np.uint8)\n",
        "del median_list, first_img, th_first\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Completed processing all {len(files)} images\\n\")\n",
        "num_layers = len(files)\n",
        "\n",
        "# Define stacking strategies (now using pre-computed accumulators)\n",
        "strategies = {\n",
        "    \"At least 1 layer\": {\n",
        "        \"mask\": lambda: (non_zero_count >= 1).astype(np.uint8) * 255,\n",
        "        \"description\": \"Pixel is 1 if non-zero in at least 1 layer\"\n",
        "    },\n",
        "    \"At least 2 layers\": {\n",
        "        \"mask\": lambda: (non_zero_count >= 2).astype(np.uint8) * 255,\n",
        "        \"description\": \"Pixel is 1 if non-zero in at least 2 layers\"\n",
        "    },\n",
        "    \"At least 3 layers\": {\n",
        "        \"mask\": lambda: (non_zero_count >= 3).astype(np.uint8) * 255,\n",
        "        \"description\": \"Pixel is 1 if non-zero in at least 3 layers\"\n",
        "    },\n",
        "    \"At least 4 layers\": {\n",
        "        \"mask\": lambda: (non_zero_count >= 4).astype(np.uint8) * 255,\n",
        "        \"description\": \"Pixel is 1 if non-zero in at least 4 layers\"\n",
        "    },\n",
        "    \"Maximum Intensity Projection (MIP)\": {\n",
        "        \"mask\": lambda: cv2.adaptiveThreshold(\n",
        "            mip_accumulator,\n",
        "            255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1\n",
        "        ),\n",
        "        \"description\": \"Take max across layers, then threshold\"\n",
        "    },\n",
        "    \"Mean Intensity Projection\": {\n",
        "        \"mask\": lambda: cv2.adaptiveThreshold(\n",
        "            mean_accumulator,\n",
        "            255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1\n",
        "        ),\n",
        "        \"description\": \"Take mean across layers, then threshold\"\n",
        "    },\n",
        "    \"Median Intensity Projection\": {\n",
        "        \"mask\": lambda: cv2.adaptiveThreshold(\n",
        "            median_accumulator,\n",
        "            255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1\n",
        "        ),\n",
        "        \"description\": \"Take median across layers, then threshold\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Calculate downsampling dimensions (once, since all images have same size)\n",
        "if height < width:\n",
        "    scale_factor = 5000 / height\n",
        "    new_height = 5000\n",
        "    new_width = int(width * scale_factor)\n",
        "else:\n",
        "    scale_factor = 5000 / width\n",
        "    new_width = 5000\n",
        "    new_height = int(height * scale_factor)\n",
        "\n",
        "# Create output directory\n",
        "output_dir = os.path.join(data_path, \"intermediate_data\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process each strategy\n",
        "results = []\n",
        "for strategy_name, strategy_info in strategies.items():\n",
        "    print(\"=\"*80)\n",
        "    print(f\"STRATEGY: {strategy_name}\")\n",
        "    print(f\"Description: {strategy_info['description']}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Generate merged mask\n",
        "    merged_mask = strategy_info[\"mask\"]()\n",
        "    \n",
        "    # Detect individual nuclei masks using findContours\n",
        "    contours, _ = cv2.findContours(merged_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    \n",
        "    # Calculate areas for each detected nucleus\n",
        "    nuclei_areas_pixels = []\n",
        "    nuclei_areas_um2 = []\n",
        "    nuclei_radii_um = []\n",
        "    \n",
        "    for contour in contours:\n",
        "        area_pixels = cv2.contourArea(contour)\n",
        "        if area_pixels > 0:  # Filter out tiny artifacts\n",
        "            nuclei_areas_pixels.append(area_pixels)\n",
        "            area_um2 = area_pixels * (pixel_size ** 2)\n",
        "            nuclei_areas_um2.append(area_um2)\n",
        "            # Approximate radius assuming circular shape: area = π * r^2\n",
        "            radius_um = np.sqrt(area_um2 / np.pi)\n",
        "            nuclei_radii_um.append(radius_um)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    if len(nuclei_areas_pixels) > 0:\n",
        "        print(f\"Number of detected nuclei: {len(nuclei_areas_pixels)}\")\n",
        "        print(f\"\\nArea statistics (pixels²):\")\n",
        "        print(f\"  Mean: {np.mean(nuclei_areas_pixels):.1f}\")\n",
        "        print(f\"  Median: {np.median(nuclei_areas_pixels):.1f}\")\n",
        "        print(f\"  Min: {np.min(nuclei_areas_pixels):.1f}\")\n",
        "        print(f\"  Max: {np.max(nuclei_areas_pixels):.1f}\")\n",
        "        print(f\"  Std: {np.std(nuclei_areas_pixels):.1f}\")\n",
        "        \n",
        "        print(f\"\\nArea statistics (μm²):\")\n",
        "        print(f\"  Mean: {np.mean(nuclei_areas_um2):.2f}\")\n",
        "        print(f\"  Median: {np.median(nuclei_areas_um2):.2f}\")\n",
        "        print(f\"  Min: {np.min(nuclei_areas_um2):.2f}\")\n",
        "        print(f\"  Max: {np.max(nuclei_areas_um2):.2f}\")\n",
        "        print(f\"  Std: {np.std(nuclei_areas_um2):.2f}\")\n",
        "        \n",
        "        print(f\"\\nRadius statistics (μm, approximated as circle):\")\n",
        "        print(f\"  Mean: {np.mean(nuclei_radii_um):.2f}\")\n",
        "        print(f\"  Median: {np.median(nuclei_radii_um):.2f}\")\n",
        "        print(f\"  Min: {np.min(nuclei_radii_um):.2f}\")\n",
        "        print(f\"  Max: {np.max(nuclei_radii_um):.2f}\")\n",
        "        print(f\"  Std: {np.std(nuclei_radii_um):.2f}\")\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            \"strategy\": strategy_name,\n",
        "            \"num_nuclei\": len(nuclei_areas_pixels),\n",
        "            \"mean_area_pixels\": np.mean(nuclei_areas_pixels),\n",
        "            \"median_area_pixels\": np.median(nuclei_areas_pixels),\n",
        "            \"mean_area_um2\": np.mean(nuclei_areas_um2),\n",
        "            \"median_area_um2\": np.median(nuclei_areas_um2),\n",
        "            \"mean_radius_um\": np.mean(nuclei_radii_um),\n",
        "            \"median_radius_um\": np.median(nuclei_radii_um)\n",
        "        })\n",
        "    else:\n",
        "        print(\"No nuclei detected!\")\n",
        "        results.append({\n",
        "            \"strategy\": strategy_name,\n",
        "            \"num_nuclei\": 0,\n",
        "            \"mean_area_pixels\": 0,\n",
        "            \"median_area_pixels\": 0,\n",
        "            \"mean_area_um2\": 0,\n",
        "            \"median_area_um2\": 0,\n",
        "            \"mean_radius_um\": 0,\n",
        "            \"median_radius_um\": 0\n",
        "        })\n",
        "    \n",
        "    # Save downsampled merged mask\n",
        "    merged_mask_downsampled = cv2.resize(merged_mask, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "    safe_filename = strategy_name.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\")\n",
        "    output_path = os.path.join(output_dir, f\"merged_mask_{safe_filename}_downsampled.png\")\n",
        "    cv2.imwrite(output_path, merged_mask_downsampled)\n",
        "    print(f\"\\nDownsampled merged mask saved to: {output_path}\")\n",
        "    print(f\"Downsampled size: {new_height} x {new_width} pixels\\n\")\n",
        "    \n",
        "    # Free memory after processing each strategy\n",
        "    del merged_mask, merged_mask_downsampled, contours\n",
        "    gc.collect()\n",
        "\n",
        "# Clean up large accumulators\n",
        "del non_zero_count, mip_accumulator, mean_accumulator, median_accumulator\n",
        "gc.collect()\n",
        "\n",
        "# Print summary table\n",
        "print(\"=\"*80)\n",
        "print(\"SUMMARY OF ALL STRATEGIES\")\n",
        "print(\"=\"*80)\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_path = os.path.join(output_dir, \"stacking_strategies_summary.csv\")\n",
        "results_df.to_csv(summary_path, index=False)\n",
        "print(f\"Summary saved to: {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5050ff38",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1839.5"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median([i for i in nuclei_areas_pixels if i > 1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30aa53e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== Dilation Benchmarking ====================#\n",
        "\n",
        "# Benchmark settings\n",
        "kernel_sizes = [3, 5]\n",
        "iterations_list = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "print(\"Starting dilation benchmarking...\\n\")\n",
        "\n",
        "for kernel_size in kernel_sizes:\n",
        "    for iterations in iterations_list:\n",
        "        print(f\"Testing: {kernel_size}x{kernel_size} kernel, {iterations} iterations\")\n",
        "        \n",
        "        # Initialize counters for this setting\n",
        "        all_areas = []  # Store areas of all detected nuclei masks\n",
        "        all_transcripts_overlap = []  # Track overlap for all transcripts\n",
        "        granule_overlap = []  # Track overlap for granule markers\n",
        "        nc_overlap = []  # Track overlap for negative controls\n",
        "        \n",
        "        # Process each z-layer\n",
        "        for j, fname in enumerate(files):\n",
        "            # Load DAPI image\n",
        "            img_path = os.path.join(data_path, \"raw_data/DAPI_images\", fname)\n",
        "            img = tifffile.imread(img_path)\n",
        "            img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "            \n",
        "            # Threshold and dilate with current settings\n",
        "            th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 49, -1)\n",
        "            kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "            th = cv2.dilate(th, kernel, iterations=iterations)\n",
        "            \n",
        "            # Detect individual nuclei masks using findContours\n",
        "            contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            \n",
        "            # Calculate areas for each detected nucleus\n",
        "            for contour in contours:\n",
        "                area_pixels = cv2.contourArea(contour)\n",
        "                if area_pixels > 0:  # Filter out tiny artifacts\n",
        "                    all_areas.append(area_pixels)\n",
        "            \n",
        "            # Map transcripts to this z-layer and check overlap\n",
        "            trans_z_mask = transcripts[\"global_z\"] == j\n",
        "            trans_z = transcripts[trans_z_mask].copy()\n",
        "            row_vals = trans_z[\"row\"].astype(int).values\n",
        "            col_vals = trans_z[\"col\"].astype(int).values\n",
        "            \n",
        "            # Avoid out-of-bounds indexing\n",
        "            height, width = th.shape\n",
        "            valid = (row_vals >= 0) & (row_vals < height) & (col_vals >= 0) & (col_vals < width)\n",
        "            row_valid = row_vals[valid]\n",
        "            col_valid = col_vals[valid]\n",
        "            \n",
        "            # Check overlap for valid transcripts\n",
        "            overlaps = np.zeros(len(trans_z), dtype=int)\n",
        "            overlaps[valid] = (th[row_valid, col_valid] != 0).astype(int)\n",
        "            \n",
        "            # Store overlap information\n",
        "            for idx, overlap in enumerate(overlaps):\n",
        "                gene = trans_z.iloc[idx][\"gene\"]\n",
        "                all_transcripts_overlap.append(overlap)\n",
        "                \n",
        "                if gene in granule_markers:\n",
        "                    granule_overlap.append(overlap)\n",
        "                if gene in nc_markers:\n",
        "                    nc_overlap.append(overlap)\n",
        "        \n",
        "        # Calculate statistics\n",
        "        if len(all_areas) > 0:\n",
        "            avg_area_pixels = np.mean(all_areas)\n",
        "            avg_area_um2 = avg_area_pixels * (pixel_size ** 2)\n",
        "            # Approximate radius assuming circular shape: area = π * r^2\n",
        "            avg_radius_um = np.sqrt(avg_area_um2 / np.pi)\n",
        "        else:\n",
        "            avg_area_pixels = 0\n",
        "            avg_area_um2 = 0\n",
        "            avg_radius_um = 0\n",
        "        \n",
        "        # Calculate extrasomatic fractions (1 - overlap fraction)\n",
        "        if len(all_transcripts_overlap) > 0:\n",
        "            extrasomatic_all = 1 - np.mean(all_transcripts_overlap)\n",
        "        else:\n",
        "            extrasomatic_all = 0\n",
        "        \n",
        "        if len(granule_overlap) > 0:\n",
        "            extrasomatic_granule = 1 - np.mean(granule_overlap)\n",
        "        else:\n",
        "            extrasomatic_granule = 0\n",
        "        \n",
        "        if len(nc_overlap) > 0:\n",
        "            extrasomatic_nc = 1 - np.mean(nc_overlap)\n",
        "        else:\n",
        "            extrasomatic_nc = 0\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            'kernel_size': kernel_size,\n",
        "            'iterations': iterations,\n",
        "            'avg_area_pixels': avg_area_pixels,\n",
        "            'avg_area_um2': avg_area_um2,\n",
        "            'avg_radius_um': avg_radius_um,\n",
        "            'extrasomatic_all_genes': extrasomatic_all,\n",
        "            'extrasomatic_granule_markers': extrasomatic_granule,\n",
        "            'extrasomatic_negative_controls': extrasomatic_nc,\n",
        "            'num_nuclei': len(all_areas),\n",
        "            'num_transcripts_all': len(all_transcripts_overlap),\n",
        "            'num_transcripts_granule': len(granule_overlap),\n",
        "            'num_transcripts_nc': len(nc_overlap)\n",
        "        })\n",
        "        \n",
        "        print(f\"  Average area: {avg_area_pixels:.1f} pixels² ({avg_area_um2:.2f} μm²), radius: {avg_radius_um:.2f} μm\")\n",
        "        print(f\"  Extrasomatic fractions: All={extrasomatic_all:.2%}, Granule={extrasomatic_granule:.2%}, NC={extrasomatic_nc:.2%}\\n\")\n",
        "\n",
        "# Convert results to DataFrame and display\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BENCHMARK RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv(os.path.join(data_path, \"intermediate_data\", \"dilation_benchmark_results.csv\"), index=False)\n",
        "print(f\"\\nResults saved to: {os.path.join(data_path, 'intermediate_data', 'dilation_benchmark_results.csv')}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AIVC-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
