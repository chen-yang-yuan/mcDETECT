{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import miniball\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_curve, auc, f1_score\n",
        "\n",
        "from model import *\n",
        "from simulate import *\n",
        "\n",
        "output_dir = \"output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for saving simulated raw data\n",
        "def write_simulated_data(points_all, out_csv, is_3d):\n",
        "    df = points_all.copy().reset_index(drop = True)\n",
        "    df[\"transcript_id\"] = df.index.astype(int)\n",
        "    out = pd.DataFrame({\"transcript_id\": df[\"transcript_id\"],\n",
        "                        \"x\": df[\"global_x\"].astype(float),\n",
        "                        \"y\": df[\"global_y\"].astype(float),\n",
        "                        \"z\": df[\"global_z\"].astype(float) if is_3d else 0.0,\n",
        "                        \"gene\": df[\"target\"].astype(str)})\n",
        "    os.makedirs(os.path.dirname(out_csv), exist_ok = True)\n",
        "    out.to_csv(out_csv, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for calculating precision, recall, accuracy, F1 score\n",
        "def calculate_metric(ground_truth_indices, matched_index):\n",
        "    \n",
        "    flattened_matches = []\n",
        "    for match in matched_index:\n",
        "        if isinstance(match, tuple):\n",
        "            flattened_matches.extend(match)\n",
        "        elif match != -1:\n",
        "            flattened_matches.append(match)\n",
        "\n",
        "    # 1. True Positives (TP): Unique ground truth points correctly detected\n",
        "    unique_matched_points = set(flattened_matches)\n",
        "    true_positives = len(unique_matched_points & ground_truth_indices)\n",
        "\n",
        "    # 2. False Positives (FP): Detections that didn\"t match any ground truth\n",
        "    false_positives = len([x for x in matched_index if x == -1])\n",
        "\n",
        "    # 3. False Negatives (FN): Ground truth points that were never matched\n",
        "    false_negatives = len(ground_truth_indices - unique_matched_points)\n",
        "\n",
        "    # 4. Total ground truth points (used for recall)\n",
        "    total_ground_truth_points = len(ground_truth_indices)\n",
        "\n",
        "    # 5. Total detections (used for accuracy)\n",
        "    total_detections = len(matched_index)\n",
        "\n",
        "    # 6. Precision\n",
        "    if true_positives + false_positives > 0:\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "    else:\n",
        "        precision = 0.0\n",
        "\n",
        "    # 7. Recall\n",
        "    if true_positives + false_negatives > 0:\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "    else:\n",
        "        recall = 0.0\n",
        "\n",
        "    # 8. Revised Accuracy\n",
        "    true_matches = len([x for x in matched_index if x != -1])  # Count of detections correctly matched\n",
        "    if total_detections > 0:\n",
        "        accuracy = true_matches / total_detections\n",
        "    else:\n",
        "        accuracy = 0.0\n",
        "    \n",
        "    # 9. F1 Score\n",
        "    if precision + recall > 0:\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0.0\n",
        "    \n",
        "    return precision, recall, accuracy, f1\n",
        "\n",
        "# Main metric calculation function\n",
        "def metric_main(tree, ground_truth_indices, sphere):\n",
        "    matched_index = []\n",
        "    for k in range(sphere.shape[0]):\n",
        "        idx = tree.query_ball_point([sphere[\"sphere_x\"].iloc[k], sphere[\"sphere_y\"].iloc[k], sphere[\"sphere_z\"].iloc[k]], sphere[\"sphere_r\"].iloc[k])\n",
        "        if len(idx) == 0:\n",
        "            matched_index.append(-1)\n",
        "        elif len(idx) == 1:\n",
        "            matched_index += idx\n",
        "        elif len(idx) > 1:\n",
        "            matched_index.append(tuple(idx))\n",
        "    return calculate_metric(ground_truth_indices, matched_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize sphere radius distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "point_type = [\"CSR\", \"Extranuclear\", \"Intranuclear\"]\n",
        "ratio = [0.5, 0.25, 0.25]\n",
        "mean_dist_extra = 1\n",
        "mean_dist_intra = 4\n",
        "beta_extra = (1, 19)\n",
        "beta_intra = (19, 1)\n",
        "\n",
        "simulate_z = True\n",
        "name = \"A\"\n",
        "density_overall = 0.16\n",
        "num_clusters_extra = 10000\n",
        "num_clusters_intra = 4000\n",
        "seed = 1\n",
        "\n",
        "for i in range(len(point_type)):\n",
        "    simulate = simulation(name = name, density = density_overall * ratio[i], shape = (2000, 2000), layer_num = 8, layer_gap = 1.5, simulate_z = simulate_z, write_path = output_dir + \"/\", seed = seed)\n",
        "    if i == 0:\n",
        "        points_CSR = simulate.simulate_CSR()\n",
        "        points_CSR[\"type\"] = [point_type[i]] * points_CSR.shape[0]\n",
        "    elif i == 1:\n",
        "        parents_cluster_extra, points_cluster_extra = simulate.simulate_cluster(num_clusters = num_clusters_extra, beta = beta_extra, mean_dist = mean_dist_extra)\n",
        "        points_cluster_extra[\"type\"] = [point_type[i]] * points_cluster_extra.shape[0]\n",
        "    elif i == 2:\n",
        "        parents_cluster_intra, points_cluster_intra = simulate.simulate_cluster(num_clusters = num_clusters_intra, beta = beta_intra, mean_dist = mean_dist_intra)\n",
        "        points_cluster_intra[\"type\"] = [point_type[i]] * points_cluster_intra.shape[0]\n",
        "points_all = pd.concat([points_CSR, points_cluster_extra, points_cluster_intra], axis = 0, ignore_index = True)\n",
        "parents_all = parents_cluster_extra\n",
        "\n",
        "def rg(df):\n",
        "    pts = df[[\"global_x\", \"global_y\", \"global_z\"]].to_numpy()\n",
        "    c = pts.mean(axis=0)\n",
        "    return np.sqrt(((pts - c)**2).sum(axis=1).mean())\n",
        "\n",
        "radii = points_cluster_intra.groupby(\"id\").apply(rg)\n",
        "med = np.nanmean(radii)\n",
        "\n",
        "area = np.pi * (radii) ** 2\n",
        "pd.DataFrame({\"id\": radii.index, \"radius\": radii, \"area\": area}).to_csv(\"output/intranuclear_area.csv\", index = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single-marker CSR and aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings\n",
        "point_type = [\"CSR\", \"Extranuclear\", \"Intranuclear\"]\n",
        "ratio = [0.5, 0.25, 0.25]\n",
        "mean_dist_extra = 1\n",
        "mean_dist_intra = 4\n",
        "beta_extra = (1, 19)\n",
        "beta_intra = (19, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main simulation loop\n",
        "dimension_settings = {\"3D\": True, \"2D\": False}\n",
        "\n",
        "marker_settings = {\"A\": {\"density\": 0.08, \"num_clusters_extra\": 5000, \"num_clusters_intra\": 2000},\n",
        "            \"B\": {\"density\": 0.04, \"num_clusters_extra\": 3000, \"num_clusters_intra\": 1200},\n",
        "            \"C\": {\"density\": 0.02, \"num_clusters_extra\": 2000, \"num_clusters_intra\": 800}}\n",
        "\n",
        "# marker_settings = {\"D\": {\"density\": 0.01, \"num_clusters_extra\": 1250, \"num_clusters_intra\": 500},\n",
        "#                    \"E\": {\"density\": 0.005, \"num_clusters_extra\": 800, \"num_clusters_intra\": 300},\n",
        "#                    \"F\": {\"density\": 0.0025, \"num_clusters_extra\": 500, \"num_clusters_intra\": 200}}\n",
        "\n",
        "seed_lst = np.arange(1, 201)\n",
        "\n",
        "for dimension, simulate_z in dimension_settings.items():\n",
        "    \n",
        "    print(f\"Running simulations for dimension: {dimension}\")\n",
        "\n",
        "    for name, params in marker_settings.items():\n",
        "        \n",
        "        print(f\"Running simulations for marker: {name}\")\n",
        "        \n",
        "        density_overall = params[\"density\"]\n",
        "        num_clusters_extra = params[\"num_clusters_extra\"]\n",
        "        num_clusters_intra = params[\"num_clusters_intra\"]\n",
        "        \n",
        "        precision_lst = []\n",
        "        recall_lst = []\n",
        "        accuracy_lst = []\n",
        "        f1_lst = []\n",
        "\n",
        "        for seed in seed_lst:\n",
        "\n",
        "            # simulate data\n",
        "            for i in range(len(point_type)):\n",
        "                simulate = simulation(name = name, density = density_overall * ratio[i], shape = (2000, 2000), layer_num = 8, layer_gap = 1.5, simulate_z = simulate_z, write_path = output_dir + \"/\", seed = seed)\n",
        "                if i == 0:\n",
        "                    points_CSR = simulate.simulate_CSR()\n",
        "                    points_CSR[\"type\"] = [point_type[i]] * points_CSR.shape[0]\n",
        "                elif i == 1:\n",
        "                    parents_cluster_extra, points_cluster_extra = simulate.simulate_cluster(num_clusters = num_clusters_extra, beta = beta_extra, mean_dist = mean_dist_extra)\n",
        "                    points_cluster_extra[\"type\"] = [point_type[i]] * points_cluster_extra.shape[0]\n",
        "                elif i == 2:\n",
        "                    parents_cluster_intra, points_cluster_intra = simulate.simulate_cluster(num_clusters = num_clusters_intra, beta = beta_intra, mean_dist = mean_dist_intra)\n",
        "                    points_cluster_intra[\"type\"] = [point_type[i]] * points_cluster_intra.shape[0]\n",
        "            points_all = pd.concat([points_CSR, points_cluster_extra, points_cluster_intra], axis = 0, ignore_index = True)\n",
        "            parents_all = parents_cluster_extra\n",
        "            \n",
        "            # save simulated data\n",
        "            write_simulated_data(points_all, f\"simulated_data/single_marker/{dimension}/{name}/seed_{seed}.csv\", simulate_z)\n",
        "            \n",
        "            # run mcDETECT\n",
        "            detect = model(shape = (2000, 2000), transcripts = points_all, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, size_thr = 4)\n",
        "            # detect = model(shape = (2000, 2000), transcripts = points_all, target_all = [\"D\", \"E\", \"F\"], eps = 1.5, in_thr = 0.25, size_thr = 4)\n",
        "            sphere = detect.dbscan_single(target_name = name)\n",
        "            \n",
        "            # find matched index\n",
        "            tree = make_tree(d1 = np.array(parents_all[\"global_x\"]), d2 = np.array(parents_all[\"global_y\"]), d3 = np.array(parents_all[\"global_z\"]))\n",
        "            ground_truth_indices = set(parents_all.index)\n",
        "            \n",
        "            # calculate all metrics\n",
        "            precision, recall, accuracy, f1 = metric_main(tree, ground_truth_indices, sphere)\n",
        "            precision_lst.append(precision)\n",
        "            recall_lst.append(recall)\n",
        "            accuracy_lst.append(accuracy)\n",
        "            f1_lst.append(f1)\n",
        "            \n",
        "            if seed % 50 == 0:\n",
        "                print(f\"{seed} out of {len(seed_lst)} iterations!\")\n",
        "\n",
        "        results_df = pd.DataFrame({\"Simulation\": seed_lst.tolist(),\n",
        "                                   \"Precision\": precision_lst,\n",
        "                                   \"Recall\": recall_lst, \n",
        "                                   \"Accuracy\": accuracy_lst,\n",
        "                                   \"F1 Score\": f1_lst})\n",
        "        results_df.to_csv(os.path.join(output_dir, f\"single_marker_{dimension}_{name}_{num_clusters_extra}_{num_clusters_intra}.csv\"), index = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-marker CSR and aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings\n",
        "name = [\"A\", \"B\", \"C\"]\n",
        "\n",
        "shape = (2000, 2000)\n",
        "layer_num = 8\n",
        "layer_gap = 1.5\n",
        "write_path = \"\"\n",
        "\n",
        "CSR_density = [0.04, 0.02, 0.01]\n",
        "\n",
        "extra_density = [0.02, 0.01, 0.005]\n",
        "extra_num_clusters = 5000\n",
        "extra_beta = (1, 19)\n",
        "extra_comp_prob = [0.4, 0.3, 0.3]\n",
        "extra_mean_dist = 1\n",
        "\n",
        "intra_density = [0.02, 0.01, 0.005]\n",
        "intra_num_clusters = 1000\n",
        "intra_beta = (19, 1)\n",
        "intra_comp_prob = [0.8, 0.1, 0.1]\n",
        "intra_mean_dist = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main simulation loop\n",
        "dimension_settings = {\"3D\": True, \"2D\": False}\n",
        "seed_lst = np.arange(1, 201)\n",
        "\n",
        "for dimension, simulate_z in dimension_settings.items():\n",
        "    \n",
        "    print(f\"Running multi-marker simulations for dimension: {dimension}\")\n",
        "    \n",
        "    precision_lst_A, recall_lst_A, accuracy_lst_A, f1_lst_A = [], [], [], []\n",
        "    precision_lst_B, recall_lst_B, accuracy_lst_B, f1_lst_B = [], [], [], []\n",
        "    precision_lst_C, recall_lst_C, accuracy_lst_C, f1_lst_C = [], [], [], []\n",
        "    precision_lst_all, recall_lst_all, accuracy_lst_all, f1_lst_all = [], [], [], []\n",
        "\n",
        "    for seed in seed_lst:\n",
        "\n",
        "        # simulate data\n",
        "        multi_simulate_extra = multi_simulation(name = name, density = extra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed)\n",
        "        parents_extra, parents_all_extra, points_extra = multi_simulate_extra.simulate_cluster(num_clusters = extra_num_clusters, beta = extra_beta, comp_prob = extra_comp_prob, mean_dist = extra_mean_dist, comp_thr = 2)\n",
        "        \n",
        "        multi_simulate_intra = multi_simulation(name = name, density = intra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 100)\n",
        "        parents_intra, parents_all_intra, points_intra = multi_simulate_intra.simulate_cluster(num_clusters = intra_num_clusters, beta = intra_beta, comp_prob = intra_comp_prob, mean_dist = intra_mean_dist, comp_thr = 2)\n",
        "        \n",
        "        simulate_A = simulation(name = name[0], density = CSR_density[0], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 200)\n",
        "        points_CSR_A = simulate_A.simulate_CSR()\n",
        "\n",
        "        simulate_B = simulation(name = name[1], density = CSR_density[1], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 300)\n",
        "        points_CSR_B = simulate_B.simulate_CSR()\n",
        "\n",
        "        simulate_C = simulation(name = name[2], density = CSR_density[2], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 400)\n",
        "        points_CSR_C = simulate_C.simulate_CSR()\n",
        "        \n",
        "        parents_all = parents_extra\n",
        "        points_all = pd.concat([points_extra, points_intra, points_CSR_A, points_CSR_B, points_CSR_C], axis = 0, ignore_index = True)\n",
        "        \n",
        "        points_A = points_all[points_all[\"target\"] == \"A\"]\n",
        "        points_B = points_all[points_all[\"target\"] == \"B\"]\n",
        "        points_C = points_all[points_all[\"target\"] == \"C\"]\n",
        "        \n",
        "        # save simulated data\n",
        "        write_simulated_data(points_all, f\"simulated_data/multi_marker/{dimension}/all/seed_{seed}.csv\", simulate_z)\n",
        "        write_simulated_data(points_A, f\"simulated_data/multi_marker/{dimension}/A/seed_{seed}.csv\", simulate_z)\n",
        "        write_simulated_data(points_B, f\"simulated_data/multi_marker/{dimension}/B/seed_{seed}.csv\", simulate_z)\n",
        "        write_simulated_data(points_C, f\"simulated_data/multi_marker/{dimension}/C/seed_{seed}.csv\", simulate_z)\n",
        "        \n",
        "        # ground truth tree and index\n",
        "        tree = make_tree(d1 = np.array(parents_all[\"global_x\"]), d2 = np.array(parents_all[\"global_y\"]), d3 = np.array(parents_all[\"global_z\"]))\n",
        "        ground_truth_indices = set(parents_all.index)\n",
        "        \n",
        "        # run mcDETECT on A/B/C/all\n",
        "        detect_A = model(shape = (2000, 2000), transcripts = points_A, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, size_thr = 4)\n",
        "        sphere_A = detect_A.dbscan_single(target_name = \"A\")\n",
        "        precision_A, recall_A, accuracy_A, f1_A = metric_main(tree, ground_truth_indices, sphere_A)\n",
        "        precision_lst_A.append(precision_A)\n",
        "        recall_lst_A.append(recall_A)\n",
        "        accuracy_lst_A.append(accuracy_A)\n",
        "        f1_lst_A.append(f1_A)\n",
        "        \n",
        "        detect_B = model(shape = (2000, 2000), transcripts = points_B, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, size_thr = 4)\n",
        "        sphere_B = detect_B.dbscan_single(target_name = \"B\")\n",
        "        precision_B, recall_B, accuracy_B, f1_B = metric_main(tree, ground_truth_indices, sphere_B)\n",
        "        precision_lst_B.append(precision_B)\n",
        "        recall_lst_B.append(recall_B)\n",
        "        accuracy_lst_B.append(accuracy_B)\n",
        "        f1_lst_B.append(f1_B)\n",
        "        \n",
        "        detect_C = model(shape = (2000, 2000), transcripts = points_C, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, size_thr = 4)\n",
        "        sphere_C = detect_C.dbscan_single(target_name = \"C\")\n",
        "        precision_C, recall_C, accuracy_C, f1_C = metric_main(tree, ground_truth_indices, sphere_C)\n",
        "        precision_lst_C.append(precision_C)\n",
        "        recall_lst_C.append(recall_C)\n",
        "        accuracy_lst_C.append(accuracy_C)\n",
        "        f1_lst_C.append(f1_C)\n",
        "        \n",
        "        detect_all = model(shape = (2000, 2000), transcripts = points_all, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, comp_thr = 2, size_thr = 4, p = 0.2)\n",
        "        sphere_all = detect_all.merge_data()\n",
        "        precision_all, recall_all, accuracy_all, f1_all = metric_main(tree, ground_truth_indices, sphere_all)\n",
        "        precision_lst_all.append(precision_all)\n",
        "        recall_lst_all.append(recall_all)\n",
        "        accuracy_lst_all.append(accuracy_all)\n",
        "        f1_lst_all.append(f1_all)\n",
        "        \n",
        "        if seed % 50 == 0:\n",
        "            print(\"{} out of {} iterations!\".format(seed, len(seed_lst)))\n",
        "    \n",
        "    pd.DataFrame({\"Simulation\": seed_lst.tolist(), \"Precision\": precision_lst_A, \"Recall\": recall_lst_A, \"Accuracy\": accuracy_lst_A, \"F1\": f1_lst_A}).to_csv(os.path.join(output_dir, f\"multi_marker_{dimension}_A_{extra_num_clusters}_{intra_num_clusters}.csv\"), index = 0)\n",
        "    pd.DataFrame({\"Simulation\": seed_lst.tolist(), \"Precision\": precision_lst_B, \"Recall\": recall_lst_B, \"Accuracy\": accuracy_lst_B, \"F1\": f1_lst_B}).to_csv(os.path.join(output_dir, f\"multi_marker_{dimension}_B_{extra_num_clusters}_{intra_num_clusters}.csv\"), index = 0)\n",
        "    pd.DataFrame({\"Simulation\": seed_lst.tolist(), \"Precision\": precision_lst_C, \"Recall\": recall_lst_C, \"Accuracy\": accuracy_lst_C, \"F1\": f1_lst_C}).to_csv(os.path.join(output_dir, f\"multi_marker_{dimension}_C_{extra_num_clusters}_{intra_num_clusters}.csv\"), index = 0)\n",
        "    pd.DataFrame({\"Simulation\": seed_lst.tolist(), \"Precision\": precision_lst_all, \"Recall\": recall_lst_all, \"Accuracy\": accuracy_lst_all, \"F1\": f1_lst_all}).to_csv(os.path.join(output_dir, f\"multi_marker_{dimension}_all_{extra_num_clusters}_{intra_num_clusters}.csv\"), index = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark parameter p in the multi-marker scenario\n",
        "\n",
        "**Note on raw count vs precision:** Raw detection count (e.g., 5137 at p=0.2) exceeds ground truth (~3000) because many detections are *split* (one aggregate detected as multiple overlapping spheres). Precision/recall use spatial matching: FP = detections with NO ground truth overlap. At p=0.2, precision ~95% and recall ~99% (same as Multi-marker CSR section), so few true FPs. The model always applies \"drop contained\" (see `remove_overlaps` in model.py); p only controls merging of overlapping-but-not-containing pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up\n",
        "name = [\"A\", \"B\", \"C\"]\n",
        "\n",
        "shape = (2000, 2000)\n",
        "layer_num = 8\n",
        "layer_gap = 1.5\n",
        "write_path = \"\"\n",
        "\n",
        "CSR_density = [0.04, 0.02, 0.01]\n",
        "\n",
        "extra_density = [0.02, 0.01, 0.005]\n",
        "extra_num_clusters = 5000\n",
        "extra_beta = (1, 19)\n",
        "extra_comp_prob = [0.4, 0.3, 0.3]\n",
        "extra_mean_dist = 1\n",
        "\n",
        "intra_density = [0.02, 0.01, 0.005]\n",
        "intra_num_clusters = 1000\n",
        "intra_beta = (19, 1)\n",
        "intra_comp_prob = [0.8, 0.1, 0.1]\n",
        "intra_mean_dist = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions for volume-, Jaccard-, and Dice-based merge (adapted from benchmark_rho.py)\n",
        "# Each has an l-like parameter (l_vol, l_jaccard, l_dice) fixed at 2.5 to match distance l=2.5\n",
        "\n",
        "def sphere_volume(r):\n",
        "    return (4.0 / 3.0) * np.pi * (r ** 3)\n",
        "\n",
        "def sphere_intersection_volume(d, r1, r2):\n",
        "    if d <= 1e-12:\n",
        "        return sphere_volume(min(r1, r2))\n",
        "    if d >= r1 + r2:\n",
        "        return 0.0\n",
        "    if d <= r2 - r1:\n",
        "        return sphere_volume(r1)\n",
        "    term = (r1 + r2 - d) ** 2 * (d**2 + 2 * d * (r1 + r2) - 3 * (r1 - r2) ** 2)\n",
        "    return (np.pi / (12 * d)) * term\n",
        "\n",
        "def get_points_in_sphere(center_xyz, radius, gene, tree_transcripts, transcript_coords, gene_per_transcript):\n",
        "    idx = np.array(tree_transcripts.query_ball_point(center_xyz, radius), dtype=np.intp)\n",
        "    if len(idx) == 0:\n",
        "        return np.empty((0, 3))\n",
        "    mask = gene_per_transcript[idx] == gene\n",
        "    return transcript_coords[idx[mask]]\n",
        "\n",
        "def _do_merge_two_spheres_sim(set_a, i, set_b, j, sphere_a, sphere_b, s, tree_transcripts, transcript_coords, gene_per_transcript):\n",
        "    ca = np.array([sphere_a[\"sphere_x\"], sphere_a[\"sphere_y\"], sphere_a[\"sphere_z\"]])\n",
        "    ra = float(sphere_a[\"sphere_r\"])\n",
        "    cb = np.array([sphere_b[\"sphere_x\"], sphere_b[\"sphere_y\"], sphere_b[\"sphere_z\"]])\n",
        "    rb = float(sphere_b[\"sphere_r\"])\n",
        "    pts_a = get_points_in_sphere(ca, ra, sphere_a[\"gene\"], tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "    pts_b = get_points_in_sphere(cb, rb, sphere_b[\"gene\"], tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "    pts_union = np.vstack([pts_a, pts_b]) if pts_a.size and pts_b.size else (pts_a if pts_a.size else pts_b)\n",
        "    if pts_union.size == 0:\n",
        "        return False\n",
        "    try:\n",
        "        new_center, r2 = miniball.get_bounding_ball(pts_union, epsilon=1e-8)\n",
        "    except Exception:\n",
        "        return False\n",
        "    new_r = np.sqrt(r2) * s\n",
        "    set_a.loc[i, \"sphere_x\"] = new_center[0]\n",
        "    set_a.loc[i, \"sphere_y\"] = new_center[1]\n",
        "    set_a.loc[i, \"sphere_z\"] = new_center[2]\n",
        "    set_a.loc[i, \"sphere_r\"] = new_r\n",
        "    set_b.drop(index=j, inplace=True)\n",
        "    return True\n",
        "\n",
        "def remove_overlaps_by_volume_sim(set_a, set_b, gamma, l_vol, s, tree_transcripts, transcript_coords, gene_per_transcript):\n",
        "    gamma_eff = gamma / l_vol\n",
        "    set_a, set_b = set_a.copy(), set_b.copy()\n",
        "    if set_a.shape[0] == 0 or set_b.shape[0] == 0:\n",
        "        return set_a, set_b\n",
        "    idx_b = make_rtree(set_b)\n",
        "    for i, sphere_a in set_a.iterrows():\n",
        "        ca = np.array([sphere_a.sphere_x, sphere_a.sphere_y, sphere_a.sphere_z])\n",
        "        ra = float(sphere_a.sphere_r)\n",
        "        va = sphere_volume(ra)\n",
        "        bounds_a = (sphere_a.sphere_x - sphere_a.sphere_r, sphere_a.sphere_y - sphere_a.sphere_r,\n",
        "                    sphere_a.sphere_x + sphere_a.sphere_r, sphere_a.sphere_y + sphere_a.sphere_r)\n",
        "        for j in idx_b.intersection(bounds_a):\n",
        "            if j not in set_b.index:\n",
        "                continue\n",
        "            sphere_b = set_b.loc[j]\n",
        "            cb = np.array([sphere_b.sphere_x, sphere_b.sphere_y, sphere_b.sphere_z])\n",
        "            rb = float(sphere_b.sphere_r)\n",
        "            vb = sphere_volume(rb)\n",
        "            d = np.linalg.norm(ca - cb)\n",
        "            if d >= ra + rb:\n",
        "                continue\n",
        "            r_small, r_large = (ra, rb) if ra <= rb else (rb, ra)\n",
        "            v_small = sphere_volume(r_small)\n",
        "            if d <= r_large - r_small:\n",
        "                if ra > rb:\n",
        "                    set_b.drop(index=j, inplace=True)\n",
        "                else:\n",
        "                    set_a.loc[i] = set_b.loc[j]\n",
        "                    set_b.drop(index=j, inplace=True)\n",
        "                continue\n",
        "            inter_vol = sphere_intersection_volume(d, r_small, r_large)\n",
        "            if inter_vol / v_small < gamma_eff:\n",
        "                continue\n",
        "            pts_a = get_points_in_sphere(ca, ra, sphere_a[\"gene\"], tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "            pts_b = get_points_in_sphere(cb, rb, sphere_b[\"gene\"], tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "            pts_union = np.vstack([pts_a, pts_b]) if pts_a.size and pts_b.size else (pts_a if pts_a.size else pts_b)\n",
        "            if pts_union.size == 0:\n",
        "                continue\n",
        "            try:\n",
        "                new_center, r2 = miniball.get_bounding_ball(pts_union, epsilon=1e-8)\n",
        "            except Exception:\n",
        "                continue\n",
        "            new_r = np.sqrt(r2) * s\n",
        "            set_a.loc[i, \"sphere_x\"], set_a.loc[i, \"sphere_y\"], set_a.loc[i, \"sphere_z\"] = new_center[0], new_center[1], new_center[2]\n",
        "            set_a.loc[i, \"sphere_r\"] = new_r\n",
        "            set_b.drop(index=j, inplace=True)\n",
        "    return set_a.reset_index(drop=True), set_b.reset_index(drop=True)\n",
        "\n",
        "def remove_overlaps_by_jaccard_sim(set_a, set_b, jaccard_thr, l_jaccard, s, tree_transcripts, transcript_coords, gene_per_transcript):\n",
        "    jaccard_eff = jaccard_thr / l_jaccard\n",
        "    set_a, set_b = set_a.copy(), set_b.copy()\n",
        "    if set_a.shape[0] == 0 or set_b.shape[0] == 0:\n",
        "        return set_a, set_b\n",
        "    idx_b = make_rtree(set_b)\n",
        "    for i, sphere_a in set_a.iterrows():\n",
        "        ca = np.array([sphere_a.sphere_x, sphere_a.sphere_y, sphere_a.sphere_z])\n",
        "        ra, va = float(sphere_a.sphere_r), sphere_volume(float(sphere_a.sphere_r))\n",
        "        bounds_a = (sphere_a.sphere_x - sphere_a.sphere_r, sphere_a.sphere_y - sphere_a.sphere_r,\n",
        "                    sphere_a.sphere_x + sphere_a.sphere_r, sphere_a.sphere_y + sphere_a.sphere_r)\n",
        "        for j in idx_b.intersection(bounds_a):\n",
        "            if j not in set_b.index:\n",
        "                continue\n",
        "            sphere_b = set_b.loc[j]\n",
        "            cb = np.array([sphere_b.sphere_x, sphere_b.sphere_y, sphere_b.sphere_z])\n",
        "            rb, vb = float(sphere_b.sphere_r), sphere_volume(float(sphere_b.sphere_r))\n",
        "            d = np.linalg.norm(ca - cb)\n",
        "            if d >= ra + rb:\n",
        "                continue\n",
        "            r_small, r_large = (ra, rb) if ra <= rb else (rb, ra)\n",
        "            if d <= r_large - r_small:\n",
        "                if ra > rb:\n",
        "                    set_b.drop(index=j, inplace=True)\n",
        "                else:\n",
        "                    set_a.loc[i] = set_b.loc[j]\n",
        "                    set_b.drop(index=j, inplace=True)\n",
        "                continue\n",
        "            inter_vol = sphere_intersection_volume(d, r_small, r_large)\n",
        "            union_vol = va + vb - inter_vol\n",
        "            if union_vol <= 0 or inter_vol / union_vol < jaccard_eff:\n",
        "                continue\n",
        "            _do_merge_two_spheres_sim(set_a, i, set_b, j, sphere_a, sphere_b, s, tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "    return set_a.reset_index(drop=True), set_b.reset_index(drop=True)\n",
        "\n",
        "def remove_overlaps_by_dice_sim(set_a, set_b, dice_thr, l_dice, s, tree_transcripts, transcript_coords, gene_per_transcript):\n",
        "    dice_eff = dice_thr / l_dice\n",
        "    set_a, set_b = set_a.copy(), set_b.copy()\n",
        "    if set_a.shape[0] == 0 or set_b.shape[0] == 0:\n",
        "        return set_a, set_b\n",
        "    idx_b = make_rtree(set_b)\n",
        "    for i, sphere_a in set_a.iterrows():\n",
        "        ca = np.array([sphere_a.sphere_x, sphere_a.sphere_y, sphere_a.sphere_z])\n",
        "        ra, va = float(sphere_a.sphere_r), sphere_volume(float(sphere_a.sphere_r))\n",
        "        bounds_a = (sphere_a.sphere_x - sphere_a.sphere_r, sphere_a.sphere_y - sphere_a.sphere_r,\n",
        "                    sphere_a.sphere_x + sphere_a.sphere_r, sphere_a.sphere_y + sphere_a.sphere_r)\n",
        "        for j in idx_b.intersection(bounds_a):\n",
        "            if j not in set_b.index:\n",
        "                continue\n",
        "            sphere_b = set_b.loc[j]\n",
        "            cb = np.array([sphere_b.sphere_x, sphere_b.sphere_y, sphere_b.sphere_z])\n",
        "            rb, vb = float(sphere_b.sphere_r), sphere_volume(float(sphere_b.sphere_r))\n",
        "            d = np.linalg.norm(ca - cb)\n",
        "            if d >= ra + rb:\n",
        "                continue\n",
        "            r_small, r_large = (ra, rb) if ra <= rb else (rb, ra)\n",
        "            if d <= r_large - r_small:\n",
        "                if ra > rb:\n",
        "                    set_b.drop(index=j, inplace=True)\n",
        "                else:\n",
        "                    set_a.loc[i] = set_b.loc[j]\n",
        "                    set_b.drop(index=j, inplace=True)\n",
        "                continue\n",
        "            inter_vol = sphere_intersection_volume(d, r_small, r_large)\n",
        "            if (va + vb) <= 0 or (2.0 * inter_vol) / (va + vb) < dice_eff:\n",
        "                continue\n",
        "            _do_merge_two_spheres_sim(set_a, i, set_b, j, sphere_a, sphere_b, s, tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "    return set_a.reset_index(drop=True), set_b.reset_index(drop=True)\n",
        "\n",
        "def merge_sphere_by_volume_sim(sphere_dict, target_names, param, l_vol, s, tree_transcripts, transcript_coords, gene_per_transcript):\n",
        "    gamma = 1.0 - param  # param 0->drop only, param 1->full\n",
        "    sphere = sphere_dict[0].copy()\n",
        "    for j in range(1, len(target_names)):\n",
        "        set_b = sphere_dict[j]\n",
        "        sphere, set_b_new = remove_overlaps_by_volume_sim(sphere, set_b, gamma, l_vol, s, tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "        sphere = pd.concat([sphere, set_b_new], ignore_index=True).reset_index(drop=True)\n",
        "    return sphere\n",
        "\n",
        "def merge_sphere_by_jaccard_sim(sphere_dict, target_names, param, l_jaccard, s, tree_transcripts, transcript_coords, gene_per_transcript):\n",
        "    jaccard_thr = 1.0 - param\n",
        "    sphere = sphere_dict[0].copy()\n",
        "    for j in range(1, len(target_names)):\n",
        "        set_b = sphere_dict[j]\n",
        "        sphere, set_b_new = remove_overlaps_by_jaccard_sim(sphere, set_b, jaccard_thr, l_jaccard, s, tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "        sphere = pd.concat([sphere, set_b_new], ignore_index=True).reset_index(drop=True)\n",
        "    return sphere\n",
        "\n",
        "def merge_sphere_by_dice_sim(sphere_dict, target_names, param, l_dice, s, tree_transcripts, transcript_coords, gene_per_transcript):\n",
        "    dice_thr = 1.0 - param\n",
        "    sphere = sphere_dict[0].copy()\n",
        "    for j in range(1, len(target_names)):\n",
        "        set_b = sphere_dict[j]\n",
        "        sphere, set_b_new = remove_overlaps_by_dice_sim(sphere, set_b, dice_thr, l_dice, s, tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "        sphere = pd.concat([sphere, set_b_new], ignore_index=True).reset_index(drop=True)\n",
        "    return sphere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark merge strategies: distance (default), volume, Jaccard, Dice\n",
        "# param 0..1 for each; l=2.5 for distance; l_vol=l_jaccard=l_dice=2.5 for others\n",
        "p_values = np.arange(0, 1.1, 0.1)\n",
        "benchmark_seeds = np.arange(1, 11)\n",
        "l_distance = l_vol = l_jaccard = l_dice = 2.5\n",
        "s_volume = 1.0\n",
        "\n",
        "def compute_avg_detections_per_GT(sphere_all, parents_all):\n",
        "    detections_per_GT = []\n",
        "    for _, gt_row in parents_all.iterrows():\n",
        "        gx, gy, gz = gt_row[\"global_x\"], gt_row[\"global_y\"], gt_row[\"global_z\"]\n",
        "        count = sum(1 for _, sphere in sphere_all.iterrows()\n",
        "                    if np.sqrt((gx - sphere[\"sphere_x\"])**2 + (gy - sphere[\"sphere_y\"])**2 + (gz - sphere[\"sphere_z\"])**2) <= sphere[\"sphere_r\"])\n",
        "        detections_per_GT.append(count)\n",
        "    return np.mean(detections_per_GT) if detections_per_GT else 0.0\n",
        "\n",
        "results_rows = []\n",
        "\n",
        "print(\"Benchmarking merge strategies (distance, volume, Jaccard, Dice)...\")\n",
        "\n",
        "for benchmark_seed in benchmark_seeds:\n",
        "    \n",
        "    multi_simulate_extra = multi_simulation(name=name, density=extra_density, shape=shape, layer_num=layer_num, layer_gap=layer_gap, simulate_z=True, write_path=write_path, seed=benchmark_seed)\n",
        "    parents_extra, _, points_extra = multi_simulate_extra.simulate_cluster(num_clusters=extra_num_clusters, beta=extra_beta, comp_prob=extra_comp_prob, mean_dist=extra_mean_dist, comp_thr=2)\n",
        "    multi_simulate_intra = multi_simulation(name=name, density=intra_density, shape=shape, layer_num=layer_num, layer_gap=layer_gap, simulate_z=True, write_path=write_path, seed=benchmark_seed + 10)\n",
        "    _, _, points_intra = multi_simulate_intra.simulate_cluster(num_clusters=intra_num_clusters, beta=intra_beta, comp_prob=intra_comp_prob, mean_dist=intra_mean_dist, comp_thr=2)\n",
        "    simulate_A = simulation(name=name[0], density=CSR_density[0], shape=shape, layer_num=layer_num, layer_gap=layer_gap, simulate_z=True, write_path=write_path, seed=benchmark_seed + 20)\n",
        "    points_CSR_A = simulate_A.simulate_CSR()\n",
        "    simulate_B = simulation(name=name[1], density=CSR_density[1], shape=shape, layer_num=layer_num, layer_gap=layer_gap, simulate_z=True, write_path=write_path, seed=benchmark_seed + 30)\n",
        "    points_CSR_B = simulate_B.simulate_CSR()\n",
        "    simulate_C = simulation(name=name[2], density=CSR_density[2], shape=shape, layer_num=layer_num, layer_gap=layer_gap, simulate_z=True, write_path=write_path, seed=benchmark_seed + 40)\n",
        "    points_CSR_C = simulate_C.simulate_CSR()\n",
        "    parents_all = parents_extra\n",
        "    points_all = pd.concat([points_extra, points_intra, points_CSR_A, points_CSR_B, points_CSR_C], axis=0, ignore_index=True)\n",
        "    \n",
        "    # Ground truth tree for precision/recall/accuracy/F1 (uncomment metric_main calls below to use)\n",
        "    tree_gt = make_tree(d1=np.array(parents_all[\"global_x\"]), d2=np.array(parents_all[\"global_y\"]), d3=np.array(parents_all[\"global_z\"]))\n",
        "    ground_truth_indices = set(parents_all.index)\n",
        "    \n",
        "    transcript_coords = points_all[[\"global_x\", \"global_y\", \"global_z\"]].values\n",
        "    gene_per_transcript = points_all[\"target\"].values\n",
        "    tree_transcripts = make_tree(d1=transcript_coords[:, 0], d2=transcript_coords[:, 1], d3=transcript_coords[:, 2])\n",
        "    \n",
        "    detect_base = model(shape=(2000, 2000), transcripts=points_all, target_all=name, eps=1.5, in_thr=0.25, comp_thr=2, size_thr=4, p=0.5, l=l_distance)\n",
        "    sphere_dict = detect_base.dbscan()\n",
        "    \n",
        "    for param in p_values:\n",
        "        \n",
        "        # Distance (default)\n",
        "        detect = model(shape=(2000, 2000), transcripts=points_all, target_all=name, eps=1.5, in_thr=0.25, comp_thr=2, size_thr=4, p=param, l=l_distance)\n",
        "        sphere_dist = detect.merge_sphere(sphere_dict)\n",
        "        # precision, recall, accuracy, f1 = metric_main(tree_gt, ground_truth_indices, sphere_dist)\n",
        "        results_rows.append({\"strategy\": \"distance\", \"param\": param, \"seed\": benchmark_seed, \"num_detections\": sphere_dist.shape[0], \"avg_detections_per_GT\": compute_avg_detections_per_GT(sphere_dist, parents_all)})\n",
        "        # To enable: uncomment metric_main above and add: \"precision\": precision, \"recall\": recall, \"accuracy\": accuracy, \"f1\": f1\n",
        "        \n",
        "        # Volume\n",
        "        sphere_vol = merge_sphere_by_volume_sim(sphere_dict, name, param, l_vol, s_volume, tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "        # precision, recall, accuracy, f1 = metric_main(tree_gt, ground_truth_indices, sphere_vol)\n",
        "        results_rows.append({\"strategy\": \"volume\", \"param\": param, \"seed\": benchmark_seed, \"num_detections\": sphere_vol.shape[0], \"avg_detections_per_GT\": compute_avg_detections_per_GT(sphere_vol, parents_all)})\n",
        "        \n",
        "        # Jaccard\n",
        "        sphere_jaccard = merge_sphere_by_jaccard_sim(sphere_dict, name, param, l_jaccard, s_volume, tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "        # precision, recall, accuracy, f1 = metric_main(tree_gt, ground_truth_indices, sphere_jaccard)\n",
        "        results_rows.append({\"strategy\": \"jaccard\", \"param\": param, \"seed\": benchmark_seed, \"num_detections\": sphere_jaccard.shape[0], \"avg_detections_per_GT\": compute_avg_detections_per_GT(sphere_jaccard, parents_all)})\n",
        "        \n",
        "        # Dice\n",
        "        sphere_dice = merge_sphere_by_dice_sim(sphere_dict, name, param, l_dice, s_volume, tree_transcripts, transcript_coords, gene_per_transcript)\n",
        "        # precision, recall, accuracy, f1 = metric_main(tree_gt, ground_truth_indices, sphere_dice)\n",
        "        results_rows.append({\"strategy\": \"dice\", \"param\": param, \"seed\": benchmark_seed, \"num_detections\": sphere_dice.shape[0], \"avg_detections_per_GT\": compute_avg_detections_per_GT(sphere_dice, parents_all)})\n",
        "        \n",
        "        print(f\"p={param} in seed {benchmark_seed} done!\")\n",
        "\n",
        "results_df = pd.DataFrame(results_rows)\n",
        "mean_df = results_df.groupby([\"strategy\", \"param\"]).agg({\"num_detections\": \"mean\", \"avg_detections_per_GT\": \"mean\"}).reset_index()\n",
        "mean_df.columns = [\"strategy\", \"param\", \"num_detections_mean\", \"avg_detections_per_GT_mean\"]\n",
        "\n",
        "# To enable precision/recall/accuracy/F1: add to agg dict: \"precision\": \"mean\", \"recall\": \"mean\", \"accuracy\": \"mean\", \"f1\": \"mean\"\n",
        "# mean_df = results_df.groupby([\"strategy\", \"param\"]).agg({\"num_detections\": \"mean\", \"avg_detections_per_GT\": \"mean\", \"precision\": \"mean\", \"recall\": \"mean\", \"accuracy\": \"mean\", \"f1\": \"mean\"}).reset_index()\n",
        "# mean_df.columns = [\"strategy\", \"param\", \"num_detections_mean\", \"avg_detections_per_GT_mean\", \"precision_mean\", \"recall_mean\", \"accuracy_mean\", \"f1_mean\"]\n",
        "\n",
        "mean_df.to_csv(os.path.join(output_dir, \"p_benchmark_multi_marker_3D_all_strategies.csv\"), index=False)\n",
        "print(f\"Saved: {os.path.join(output_dir, 'p_benchmark_multi_marker_3D_all_strategies.csv')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark ratio between CSR, extranuclear, and intranuclear aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark 1: Vary CSR ratio, keep extra and intra-nuclear ratios identical\n",
        "# Only use A, B, C markers and 3D case\n",
        "\n",
        "# Original marker settings for A, B, C\n",
        "marker_settings_original = {\"A\": {\"density\": 0.08, \"num_clusters_extra\": 5000, \"num_clusters_intra\": 2000},\n",
        "                            \"B\": {\"density\": 0.04, \"num_clusters_extra\": 3000, \"num_clusters_intra\": 1200},\n",
        "                            \"C\": {\"density\": 0.02, \"num_clusters_extra\": 2000, \"num_clusters_intra\": 800}}\n",
        "\n",
        "# Original ratio\n",
        "original_ratio = [0.5, 0.25, 0.25]  # [CSR, Extra, Intra]\n",
        "original_extra_ratio = original_ratio[1]\n",
        "original_intra_ratio = original_ratio[2]\n",
        "\n",
        "# Test 5 different CSR ratios\n",
        "csr_ratios = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "\n",
        "# Settings\n",
        "point_type = [\"CSR\", \"Extranuclear\", \"Intranuclear\"]\n",
        "mean_dist_extra = 1\n",
        "mean_dist_intra = 4\n",
        "beta_extra = (1, 19)\n",
        "beta_intra = (19, 1)\n",
        "simulate_z = True  # 3D only\n",
        "shape_area = 2000 * 2000  # Used for scaling cluster counts\n",
        "\n",
        "seed_lst = np.arange(1, 101)\n",
        "\n",
        "print(\"Benchmark 1: Varying CSR ratio (keeping extra = intra)\")\n",
        "print(f\"Testing CSR ratios: {csr_ratios}\")\n",
        "\n",
        "for csr_ratio in csr_ratios:\n",
        "    # Calculate extra and intra ratios (they must be equal and sum with CSR to 1)\n",
        "    remaining_ratio = 1.0 - csr_ratio\n",
        "    extra_ratio = remaining_ratio / 2.0\n",
        "    intra_ratio = remaining_ratio / 2.0\n",
        "    ratio = [csr_ratio, extra_ratio, intra_ratio]\n",
        "    \n",
        "    print(f\"\\nCSR ratio: {csr_ratio:.2f}, Extra ratio: {extra_ratio:.2f}, Intra ratio: {intra_ratio:.2f}\")\n",
        "    \n",
        "    for name, params_original in marker_settings_original.items():\n",
        "        density_overall = params_original[\"density\"]\n",
        "        \n",
        "        # Calculate scaling factors to maintain approximately same number of transcripts per cluster\n",
        "        # Points per cluster â‰ˆ (density * shape_area) / num_clusters\n",
        "        # To keep points per cluster constant: num_clusters must scale proportionally with density change\n",
        "        # num_clusters_new / num_clusters_original = new_density / original_density = new_ratio / original_ratio\n",
        "        extra_scale_factor = extra_ratio / original_extra_ratio\n",
        "        intra_scale_factor = intra_ratio / original_intra_ratio\n",
        "        \n",
        "        num_clusters_extra = int(params_original[\"num_clusters_extra\"] * extra_scale_factor)\n",
        "        num_clusters_intra = int(params_original[\"num_clusters_intra\"] * intra_scale_factor)\n",
        "        \n",
        "        print(f\"  Marker {name}: num_clusters_extra={num_clusters_extra}, num_clusters_intra={num_clusters_intra}\")\n",
        "        \n",
        "        precision_lst = []\n",
        "        recall_lst = []\n",
        "        accuracy_lst = []\n",
        "        f1_lst = []\n",
        "        \n",
        "        for seed in seed_lst:\n",
        "            # Simulate data\n",
        "            for i in range(len(point_type)):\n",
        "                simulate = simulation(name=name, density=density_overall * ratio[i], shape=(2000, 2000), \n",
        "                                     layer_num=8, layer_gap=1.5, simulate_z=simulate_z, \n",
        "                                     write_path=output_dir + \"/\", seed=seed)\n",
        "                if i == 0:\n",
        "                    points_CSR = simulate.simulate_CSR()\n",
        "                    points_CSR[\"type\"] = [point_type[i]] * points_CSR.shape[0]\n",
        "                elif i == 1:\n",
        "                    parents_cluster_extra, points_cluster_extra = simulate.simulate_cluster(\n",
        "                        num_clusters=num_clusters_extra, beta=beta_extra, mean_dist=mean_dist_extra)\n",
        "                    points_cluster_extra[\"type\"] = [point_type[i]] * points_cluster_extra.shape[0]\n",
        "                elif i == 2:\n",
        "                    parents_cluster_intra, points_cluster_intra = simulate.simulate_cluster(\n",
        "                        num_clusters=num_clusters_intra, beta=beta_intra, mean_dist=mean_dist_intra)\n",
        "                    points_cluster_intra[\"type\"] = [point_type[i]] * points_cluster_intra.shape[0]\n",
        "            \n",
        "            points_all = pd.concat([points_CSR, points_cluster_extra, points_cluster_intra], \n",
        "                                  axis=0, ignore_index=True)\n",
        "            parents_all = parents_cluster_extra\n",
        "            \n",
        "            # Run mcDETECT\n",
        "            detect = model(shape=(2000, 2000), transcripts=points_all, target_all=[\"A\", \"B\", \"C\"], \n",
        "                          eps=1.5, in_thr=0.25, size_thr=4)\n",
        "            sphere = detect.dbscan_single(target_name=name)\n",
        "            \n",
        "            # Find matched index\n",
        "            tree = make_tree(d1=np.array(parents_all[\"global_x\"]), \n",
        "                           d2=np.array(parents_all[\"global_y\"]), \n",
        "                           d3=np.array(parents_all[\"global_z\"]))\n",
        "            ground_truth_indices = set(parents_all.index)\n",
        "            \n",
        "            # Calculate all metrics\n",
        "            precision, recall, accuracy, f1 = metric_main(tree, ground_truth_indices, sphere)\n",
        "            precision_lst.append(precision)\n",
        "            recall_lst.append(recall)\n",
        "            accuracy_lst.append(accuracy)\n",
        "            f1_lst.append(f1)\n",
        "            \n",
        "            if seed % 25 == 0:\n",
        "                print(f\"    {seed} out of {len(seed_lst)} iterations!\")\n",
        "        \n",
        "        # Save results\n",
        "        results_df = pd.DataFrame({\n",
        "            \"Simulation\": seed_lst.tolist(),\n",
        "            \"Precision\": precision_lst,\n",
        "            \"Recall\": recall_lst,\n",
        "            \"Accuracy\": accuracy_lst,\n",
        "            \"F1_Score\": f1_lst,\n",
        "            \"CSR_ratio\": [csr_ratio] * len(seed_lst),\n",
        "            \"Extra_ratio\": [extra_ratio] * len(seed_lst),\n",
        "            \"Intra_ratio\": [intra_ratio] * len(seed_lst)\n",
        "        })\n",
        "        filename = f\"benchmark_ratio_csr_{name}_csr{csr_ratio:.2f}_extra{extra_ratio:.2f}_intra{intra_ratio:.2f}.csv\"\n",
        "        results_df.to_csv(os.path.join(output_dir, filename), index=0)\n",
        "        \n",
        "        print(f\"  Marker {name}: Mean Precision={np.mean(precision_lst):.4f}, \"\n",
        "              f\"Recall={np.mean(recall_lst):.4f}, F1={np.mean(f1_lst):.4f}\")\n",
        "\n",
        "print(\"\\nBenchmark 1 completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark 2: Fix CSR at 0.5, vary the proportion of extra- and intra-nuclear ratios\n",
        "# Only use A, B, C markers and 3D case\n",
        "\n",
        "# Original marker settings for A, B, C\n",
        "marker_settings_original = {\"A\": {\"density\": 0.08, \"num_clusters_extra\": 5000, \"num_clusters_intra\": 2000},\n",
        "                            \"B\": {\"density\": 0.04, \"num_clusters_extra\": 3000, \"num_clusters_intra\": 1200},\n",
        "                            \"C\": {\"density\": 0.02, \"num_clusters_extra\": 2000, \"num_clusters_intra\": 800}}\n",
        "\n",
        "# Original ratio\n",
        "original_ratio = [0.5, 0.25, 0.25]  # [CSR, Extra, Intra]\n",
        "original_extra_ratio = original_ratio[1]\n",
        "original_intra_ratio = original_ratio[2]\n",
        "\n",
        "# Fix CSR at 0.5, test 5 different extra/intra splits\n",
        "# Since CSR = 0.5, extra + intra must sum to 0.5\n",
        "csr_ratio_fixed = 0.5\n",
        "extra_intra_splits = [\n",
        "    (0.4, 0.1),   # Extra-heavy\n",
        "    (0.325, 0.175),\n",
        "    (0.25, 0.25),  # Equal (original)\n",
        "    (0.175, 0.325),\n",
        "    (0.1, 0.4)    # Intra-heavy\n",
        "]\n",
        "\n",
        "# Settings\n",
        "point_type = [\"CSR\", \"Extranuclear\", \"Intranuclear\"]\n",
        "mean_dist_extra = 1\n",
        "mean_dist_intra = 4\n",
        "beta_extra = (1, 19)\n",
        "beta_intra = (19, 1)\n",
        "simulate_z = True  # 3D only\n",
        "shape_area = 2000 * 2000  # Used for scaling cluster counts\n",
        "\n",
        "seed_lst = np.arange(1, 101)\n",
        "\n",
        "print(\"Benchmark 2: Varying extra/intra ratio (fixing CSR at 0.5)\")\n",
        "print(f\"Testing extra/intra splits: {extra_intra_splits}\")\n",
        "\n",
        "for extra_ratio, intra_ratio in extra_intra_splits:\n",
        "    ratio = [csr_ratio_fixed, extra_ratio, intra_ratio]\n",
        "    \n",
        "    print(f\"\\nCSR ratio: {csr_ratio_fixed:.2f}, Extra ratio: {extra_ratio:.2f}, Intra ratio: {intra_ratio:.2f}\")\n",
        "    \n",
        "    for name, params_original in marker_settings_original.items():\n",
        "        density_overall = params_original[\"density\"]\n",
        "        \n",
        "        # Calculate scaling factors to maintain approximately same number of transcripts per cluster\n",
        "        # Points per cluster â‰ˆ (density * shape_area) / num_clusters\n",
        "        # To keep points per cluster constant: num_clusters must scale proportionally with density change\n",
        "        # num_clusters_new / num_clusters_original = new_density / original_density = new_ratio / original_ratio\n",
        "        extra_scale_factor = extra_ratio / original_extra_ratio\n",
        "        intra_scale_factor = intra_ratio / original_intra_ratio\n",
        "        \n",
        "        num_clusters_extra = int(params_original[\"num_clusters_extra\"] * extra_scale_factor)\n",
        "        num_clusters_intra = int(params_original[\"num_clusters_intra\"] * intra_scale_factor)\n",
        "        \n",
        "        print(f\"  Marker {name}: num_clusters_extra={num_clusters_extra}, num_clusters_intra={num_clusters_intra}\")\n",
        "        \n",
        "        precision_lst = []\n",
        "        recall_lst = []\n",
        "        accuracy_lst = []\n",
        "        f1_lst = []\n",
        "        \n",
        "        for seed in seed_lst:\n",
        "            # Simulate data\n",
        "            for i in range(len(point_type)):\n",
        "                simulate = simulation(name=name, density=density_overall * ratio[i], shape=(2000, 2000), \n",
        "                                     layer_num=8, layer_gap=1.5, simulate_z=simulate_z, \n",
        "                                     write_path=output_dir + \"/\", seed=seed)\n",
        "                if i == 0:\n",
        "                    points_CSR = simulate.simulate_CSR()\n",
        "                    points_CSR[\"type\"] = [point_type[i]] * points_CSR.shape[0]\n",
        "                elif i == 1:\n",
        "                    parents_cluster_extra, points_cluster_extra = simulate.simulate_cluster(\n",
        "                        num_clusters=num_clusters_extra, beta=beta_extra, mean_dist=mean_dist_extra)\n",
        "                    points_cluster_extra[\"type\"] = [point_type[i]] * points_cluster_extra.shape[0]\n",
        "                elif i == 2:\n",
        "                    parents_cluster_intra, points_cluster_intra = simulate.simulate_cluster(\n",
        "                        num_clusters=num_clusters_intra, beta=beta_intra, mean_dist=mean_dist_intra)\n",
        "                    points_cluster_intra[\"type\"] = [point_type[i]] * points_cluster_intra.shape[0]\n",
        "            \n",
        "            points_all = pd.concat([points_CSR, points_cluster_extra, points_cluster_intra], \n",
        "                                  axis=0, ignore_index=True)\n",
        "            parents_all = parents_cluster_extra\n",
        "            \n",
        "            # Run mcDETECT\n",
        "            detect = model(shape=(2000, 2000), transcripts=points_all, target_all=[\"A\", \"B\", \"C\"], \n",
        "                          eps=1.5, in_thr=0.25, size_thr=4)\n",
        "            sphere = detect.dbscan_single(target_name=name)\n",
        "            \n",
        "            # Find matched index\n",
        "            tree = make_tree(d1=np.array(parents_all[\"global_x\"]), \n",
        "                           d2=np.array(parents_all[\"global_y\"]), \n",
        "                           d3=np.array(parents_all[\"global_z\"]))\n",
        "            ground_truth_indices = set(parents_all.index)\n",
        "            \n",
        "            # Calculate all metrics\n",
        "            precision, recall, accuracy, f1 = metric_main(tree, ground_truth_indices, sphere)\n",
        "            precision_lst.append(precision)\n",
        "            recall_lst.append(recall)\n",
        "            accuracy_lst.append(accuracy)\n",
        "            f1_lst.append(f1)\n",
        "            \n",
        "            if seed % 25 == 0:\n",
        "                print(f\"    {seed} out of {len(seed_lst)} iterations!\")\n",
        "        \n",
        "        # Save results\n",
        "        results_df = pd.DataFrame({\n",
        "            \"Simulation\": seed_lst.tolist(),\n",
        "            \"Precision\": precision_lst,\n",
        "            \"Recall\": recall_lst,\n",
        "            \"Accuracy\": accuracy_lst,\n",
        "            \"F1_Score\": f1_lst,\n",
        "            \"CSR_ratio\": [csr_ratio_fixed] * len(seed_lst),\n",
        "            \"Extra_ratio\": [extra_ratio] * len(seed_lst),\n",
        "            \"Intra_ratio\": [intra_ratio] * len(seed_lst)\n",
        "        })\n",
        "        filename = f\"benchmark_ratio_fixedcsr_{name}_csr{csr_ratio_fixed:.2f}_extra{extra_ratio:.3f}_intra{intra_ratio:.3f}.csv\"\n",
        "        results_df.to_csv(os.path.join(output_dir, filename), index=0)\n",
        "        \n",
        "        print(f\"  Marker {name}: Mean Precision={np.mean(precision_lst):.4f}, \"\n",
        "              f\"Recall={np.mean(recall_lst):.4f}, F1={np.mean(f1_lst):.4f}\")\n",
        "\n",
        "print(\"\\nBenchmark 2 completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mcDETECT-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
