{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from model import *\n",
    "from simulate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating precision, recall, and accuracy\n",
    "def calculate_metric(ground_truth_indices, matched_index):\n",
    "    \n",
    "    flattened_matches = []\n",
    "    for match in matched_index:\n",
    "        if isinstance(match, tuple):\n",
    "            flattened_matches.extend(match)\n",
    "        elif match != -1:\n",
    "            flattened_matches.append(match)\n",
    "\n",
    "    # 1. True Positives (TP): Unique ground truth points correctly detected\n",
    "    unique_matched_points = set(flattened_matches)\n",
    "    true_positives = len(unique_matched_points & ground_truth_indices)\n",
    "\n",
    "    # 2. False Positives (FP): Detections that didn't match any ground truth\n",
    "    false_positives = len([x for x in matched_index if x == -1])\n",
    "\n",
    "    # 3. False Negatives (FN): Ground truth points that were never matched\n",
    "    false_negatives = len(ground_truth_indices - unique_matched_points)\n",
    "\n",
    "    # 4. Total ground truth points (used for recall)\n",
    "    total_ground_truth_points = len(ground_truth_indices)\n",
    "\n",
    "    # 5. Total detections (used for accuracy)\n",
    "    total_detections = len(matched_index)\n",
    "\n",
    "    # 6. Precision\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    # 7. Recall\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    # 8. Revised Accuracy\n",
    "    true_matches = len([x for x in matched_index if x != -1])  # Count of detections correctly matched\n",
    "    accuracy = true_matches / total_detections\n",
    "    \n",
    "    return precision, recall, accuracy\n",
    "\n",
    "\n",
    "def metric_main(tree, ground_truth_indices, sphere):\n",
    "    matched_index = []\n",
    "    for k in range(sphere.shape[0]):\n",
    "        idx = tree.query_ball_point([sphere['sphere_x'].iloc[k], sphere['sphere_y'].iloc[k], sphere['sphere_z'].iloc[k]], sphere['sphere_r'].iloc[k])\n",
    "        if len(idx) == 0:\n",
    "            matched_index.append(-1)\n",
    "        elif len(idx) == 1:\n",
    "            matched_index += idx\n",
    "        elif len(idx) > 1:\n",
    "            matched_index.append(tuple(idx))\n",
    "    return calculate_metric(ground_truth_indices, matched_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-marker CSR and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point types and their ratios\n",
    "point_type = ['CSR', 'Extranuclear', 'Intranuclear']\n",
    "ratio = [0.5, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean distances for extranuclear and intranuclear aggregation\n",
    "mean_dist_extra = 1\n",
    "mean_dist_intra = 3.5\n",
    "\n",
    "# Mean in-nucleus ratio for extranuclear and intranuclear aggregation\n",
    "beta_extra = (2, 8)\n",
    "beta_intra = (8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "name = 'A'\n",
    "density_overall = 0.08\n",
    "num_clusters_extra = 5000\n",
    "num_clusters_intra = 2000\n",
    "\n",
    "# name = 'B'\n",
    "# density_overall = 0.04\n",
    "# num_clusters_extra = 3000\n",
    "# num_clusters_intra = 1200\n",
    "\n",
    "# name = 'C'\n",
    "# density_overall = 0.02\n",
    "# num_clusters_extra = 2000\n",
    "# num_clusters_intra = 800\n",
    "\n",
    "seed_lst = np.arange(1, 201)\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "accuracy_lst = []\n",
    "\n",
    "for seed in seed_lst:\n",
    "\n",
    "    # simulate data\n",
    "    for i in range(len(point_type)):\n",
    "        simulate = simulation(name = name, density = density_overall * ratio[i], shape = (2000, 2000), layer_num = 8, layer_gap = 1.5, simulate_z = True, write_path = 'main_output/', seed = seed)\n",
    "        # simulate = simulation(name = name, density = density_overall * ratio[i], shape = (2000, 2000), layer_num = 8, layer_gap = 1.5, simulate_z = False, write_path = 'main_output/', seed = seed)\n",
    "        if i == 0:\n",
    "            points_CSR = simulate.simulate_CSR()\n",
    "            points_CSR['type'] = [point_type[i]] * points_CSR.shape[0]\n",
    "        elif i == 1:\n",
    "            parents_cluster_extra, points_cluster_extra = simulate.simulate_cluster(num_clusters = num_clusters_extra, beta = beta_extra, mean_dist = mean_dist_extra)\n",
    "            points_cluster_extra['type'] = [point_type[i]] * points_cluster_extra.shape[0]\n",
    "        elif i == 2:\n",
    "            parents_cluster_intra, points_cluster_intra = simulate.simulate_cluster(num_clusters = num_clusters_intra, beta = beta_intra, mean_dist = mean_dist_intra)\n",
    "            points_cluster_intra['type'] = [point_type[i]] * points_cluster_intra.shape[0]\n",
    "    points_all = pd.concat([points_CSR, points_cluster_extra, points_cluster_intra], axis = 0, ignore_index = True)\n",
    "    parents_all = parents_cluster_extra\n",
    "    \n",
    "    # run mcDETECT\n",
    "    detect = model(shape = (2000, 2000), transcripts = points_all, target_all = ['A', 'B', 'C'], eps = 1.5, in_thr = 0.5, size_thr = 3.5)\n",
    "    sphere = detect.dbscan_single(target_name = name)\n",
    "    \n",
    "    # find matched index\n",
    "    tree = make_tree(d1 = np.array(parents_all['global_x']), d2 = np.array(parents_all['global_y']), d3 = np.array(parents_all['global_z']))\n",
    "    ground_truth_indices = set(parents_all.index)\n",
    "    \n",
    "    # calculate precision, recall, and accuracy\n",
    "    precision, recall, accuracy = metric_main(tree, ground_truth_indices, sphere)\n",
    "    precision_lst.append(precision)\n",
    "    recall_lst.append(recall)\n",
    "    accuracy_lst.append(accuracy)\n",
    "    \n",
    "    if seed % 50 == 0:\n",
    "        print('{} out of {} iterations!'.format(seed, len(seed_lst)))\n",
    "\n",
    "pd.DataFrame({'Simulation': seed_lst.tolist(), 'Precision': precision_lst, 'Recall': recall_lst, 'Accuracy': accuracy_lst}).to_csv('main_output/single_marker_3D/' + name + '_{}_{}.csv'.format(num_clusters_extra, num_clusters_intra), index = 0)\n",
    "# pd.DataFrame({'Simulation': seed_lst.tolist(), 'Precision': precision_lst, 'Recall': recall_lst, 'Accuracy': accuracy_lst}).to_csv('main_output/single_marker_2D/' + name + '_{}_{}.csv'.format(num_clusters_extra, num_clusters_intra), index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-marker aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "name = ['A', 'B', 'C']\n",
    "\n",
    "shape = (2000, 2000)\n",
    "layer_num = 8\n",
    "layer_gap = 1.5\n",
    "write_path = ''\n",
    "\n",
    "CSR_density = [0.04, 0.02, 0.01]\n",
    "\n",
    "extra_density = [0.02, 0.01, 0.005]\n",
    "extra_num_clusters = 5000\n",
    "extra_beta = (2, 8)\n",
    "extra_comp_prob = [0.4, 0.3, 0.3]\n",
    "extra_mean_dist = 1\n",
    "\n",
    "intra_density = [0.02, 0.01, 0.005]\n",
    "intra_num_clusters = 1000\n",
    "intra_beta = (8, 2)\n",
    "intra_comp_prob = [0.8, 0.1, 0.1]\n",
    "intra_mean_dist = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "simulate_z = True\n",
    "\n",
    "if simulate_z:\n",
    "    setting = '3D'\n",
    "else:\n",
    "    setting = '2D'\n",
    "\n",
    "seed_lst = np.arange(1, 201)\n",
    "precision_lst_A, recall_lst_A, accuracy_lst_A = [], [], []\n",
    "precision_lst_B, recall_lst_B, accuracy_lst_B = [], [], []\n",
    "precision_lst_C, recall_lst_C, accuracy_lst_C = [], [], []\n",
    "precision_lst_all, recall_lst_all, accuracy_lst_all = [], [], []\n",
    "\n",
    "for seed in seed_lst:\n",
    "\n",
    "    # simulate data\n",
    "    multi_simulate_extra = multi_simulation(name = name, density = extra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed)\n",
    "    parents_extra, parents_all_extra, points_extra = multi_simulate_extra.simulate_cluster(num_clusters = extra_num_clusters, beta = extra_beta, comp_prob = extra_comp_prob, mean_dist = extra_mean_dist, comp_thr = 1)\n",
    "    \n",
    "    multi_simulate_intra = multi_simulation(name = name, density = intra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 100)\n",
    "    parents_intra, parents_all_intra, points_intra = multi_simulate_intra.simulate_cluster(num_clusters = intra_num_clusters, beta = intra_beta, comp_prob = intra_comp_prob, mean_dist = intra_mean_dist, comp_thr = 1)\n",
    "    \n",
    "    simulate_A = simulation(name = name[0], density = CSR_density[0], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 200)\n",
    "    points_CSR_A = simulate_A.simulate_CSR()\n",
    "\n",
    "    simulate_B = simulation(name = name[1], density = CSR_density[1], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 300)\n",
    "    points_CSR_B = simulate_B.simulate_CSR()\n",
    "\n",
    "    simulate_C = simulation(name = name[2], density = CSR_density[2], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 400)\n",
    "    points_CSR_C = simulate_C.simulate_CSR()\n",
    "    \n",
    "    parents_all = parents_extra\n",
    "    points_all = pd.concat([points_extra, points_intra, points_CSR_A, points_CSR_B, points_CSR_C], axis = 0, ignore_index = True)\n",
    "    \n",
    "    points_A = points_all[points_all['target'] == 'A']\n",
    "    points_B = points_all[points_all['target'] == 'B']\n",
    "    points_C = points_all[points_all['target'] == 'C']\n",
    "    \n",
    "    # ground truth tree and index\n",
    "    tree = make_tree(d1 = np.array(parents_all['global_x']), d2 = np.array(parents_all['global_y']), d3 = np.array(parents_all['global_z']))\n",
    "    ground_truth_indices = set(parents_all.index)\n",
    "    \n",
    "    # run mcDETECT on A/B/C/all\n",
    "    detect_A = model(shape = (2000, 2000), transcripts = points_A, target_all = ['A', 'B', 'C'], eps = 1.5, in_thr = 0.5, size_thr = 3.5)\n",
    "    sphere_A = detect_A.dbscan_single(target_name = 'A')\n",
    "    precision_A, recall_A, accuracy_A = metric_main(tree, ground_truth_indices, sphere_A)\n",
    "    precision_lst_A.append(precision_A)\n",
    "    recall_lst_A.append(recall_A)\n",
    "    accuracy_lst_A.append(accuracy_A)\n",
    "    \n",
    "    detect_B = model(shape = (2000, 2000), transcripts = points_B, target_all = ['A', 'B', 'C'], eps = 1.5, in_thr = 0.5, size_thr = 3.5)\n",
    "    sphere_B = detect_B.dbscan_single(target_name = 'B')\n",
    "    precision_B, recall_B, accuracy_B = metric_main(tree, ground_truth_indices, sphere_B)\n",
    "    precision_lst_B.append(precision_B)\n",
    "    recall_lst_B.append(recall_B)\n",
    "    accuracy_lst_B.append(accuracy_B)\n",
    "    \n",
    "    detect_C = model(shape = (2000, 2000), transcripts = points_C, target_all = ['A', 'B', 'C'], eps = 1.5, in_thr = 0.5, size_thr = 3.5)\n",
    "    sphere_C = detect_C.dbscan_single(target_name = 'C')\n",
    "    precision_C, recall_C, accuracy_C = metric_main(tree, ground_truth_indices, sphere_C)\n",
    "    precision_lst_C.append(precision_C)\n",
    "    recall_lst_C.append(recall_C)\n",
    "    accuracy_lst_C.append(accuracy_C)\n",
    "    \n",
    "    detect_all = model(shape = (2000, 2000), transcripts = points_all, target_all = ['A', 'B', 'C'], eps = 1.5, in_thr = 0.5, comp_thr = 1, size_thr = 3.5, p = 0.5)\n",
    "    sphere_all = detect_all.merge_data()\n",
    "    precision_all, recall_all, accuracy_all = metric_main(tree, ground_truth_indices, sphere_all)\n",
    "    precision_lst_all.append(precision_all)\n",
    "    recall_lst_all.append(recall_all)\n",
    "    accuracy_lst_all.append(accuracy_all)\n",
    "    \n",
    "    print('{} out of {} iterations!'.format(seed, len(seed_lst)))\n",
    "\n",
    "pd.DataFrame({'Simulation': seed_lst.tolist(), 'Precision': precision_lst_A, 'Recall': recall_lst_A, 'Accuracy': accuracy_lst_A}).to_csv('main_output/multi_marker_{}/A_{}_{}.csv'.format(setting, extra_num_clusters, intra_num_clusters), index = 0)\n",
    "pd.DataFrame({'Simulation': seed_lst.tolist(), 'Precision': precision_lst_B, 'Recall': recall_lst_B, 'Accuracy': accuracy_lst_B}).to_csv('main_output/multi_marker_{}/B_{}_{}.csv'.format(setting, extra_num_clusters, intra_num_clusters), index = 0)\n",
    "pd.DataFrame({'Simulation': seed_lst.tolist(), 'Precision': precision_lst_C, 'Recall': recall_lst_C, 'Accuracy': accuracy_lst_C}).to_csv('main_output/multi_marker_{}/C_{}_{}.csv'.format(setting, extra_num_clusters, intra_num_clusters), index = 0)\n",
    "pd.DataFrame({'Simulation': seed_lst.tolist(), 'Precision': precision_lst_all, 'Recall': recall_lst_all, 'Accuracy': accuracy_lst_all}).to_csv('main_output/multi_marker_{}/all_{}_{}.csv'.format(setting, extra_num_clusters, intra_num_clusters), index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-marker aggregation (20 markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_comp = 20\n",
    "comp_list = np.arange(1, max_comp + 1)\n",
    "\n",
    "extra_alpha, extra_beta_param = 5, 3  # not to be confused with your beta distribution for in-nucleus ratio\n",
    "x_extra = comp_list / max_comp  # scale to [0,1]\n",
    "pdf_extra = (x_extra**(extra_alpha - 1)) * ((1 - x_extra)**(extra_beta_param - 1))\n",
    "extra_comp_prob = pdf_extra / pdf_extra.sum()\n",
    "\n",
    "intra_alpha, intra_beta_param = 2, 5\n",
    "x_intra = comp_list / max_comp\n",
    "pdf_intra = (x_intra**(intra_alpha - 1)) * ((1 - x_intra)**(intra_beta_param - 1))\n",
    "intra_comp_prob = pdf_intra / pdf_intra.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "num_A, num_B, num_C = 7, 7, 6\n",
    "\n",
    "name = [f\"A_{i}\" for i in range(1, num_A + 1)] + [f\"B_{i}\" for i in range(1, num_B + 1)] + [f\"C_{i}\" for i in range(1, num_C + 1)]\n",
    "\n",
    "CSR_density_base = {'A': 0.04, 'B': 0.02, 'C': 0.01}\n",
    "extra_density_base = {'A': 0.02, 'B': 0.01, 'C': 0.005}\n",
    "intra_density_base = {'A': 0.02, 'B': 0.01, 'C': 0.005}\n",
    "\n",
    "CSR_density = []\n",
    "extra_density = []\n",
    "intra_density = []\n",
    "\n",
    "for marker in name:\n",
    "    t = marker[0]\n",
    "    CSR_density.append(CSR_density_base[t])\n",
    "    extra_density.append(extra_density_base[t])\n",
    "    intra_density.append(intra_density_base[t])\n",
    "\n",
    "shape = (2000, 2000)\n",
    "layer_num = 8\n",
    "layer_gap = 1.5\n",
    "write_path = ''\n",
    "\n",
    "extra_num_clusters = 5000\n",
    "extra_beta = (2, 8)\n",
    "extra_mean_dist = 1\n",
    "\n",
    "intra_num_clusters = 1000\n",
    "intra_beta = (8, 2)\n",
    "intra_mean_dist = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 out of 10 iterations!\n",
      "20 out of 10 iterations!\n",
      "30 out of 10 iterations!\n",
      "40 out of 10 iterations!\n",
      "50 out of 10 iterations!\n",
      "60 out of 10 iterations!\n",
      "70 out of 10 iterations!\n",
      "80 out of 10 iterations!\n",
      "90 out of 10 iterations!\n",
      "100 out of 10 iterations!\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "seed_lst = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "simulate_z = True\n",
    "\n",
    "if simulate_z:\n",
    "    setting = '3D'\n",
    "else:\n",
    "    setting = '2D'\n",
    "\n",
    "precision_dict = {m: [] for m in name}\n",
    "recall_dict = {m: [] for m in name}\n",
    "accuracy_dict = {m: [] for m in name}\n",
    "\n",
    "precision_lst_all, recall_lst_all, accuracy_lst_all = [], [], []\n",
    "\n",
    "for seed in seed_lst:\n",
    "\n",
    "    # simulate data\n",
    "    multi_simulate_extra = multi_simulation(name = name, density = extra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed)\n",
    "    parents_extra, parents_all_extra, points_extra = multi_simulate_extra.simulate_cluster(num_clusters = extra_num_clusters, beta = extra_beta, comp_prob = extra_comp_prob, mean_dist = extra_mean_dist, comp_thr = 1)\n",
    "    \n",
    "    multi_simulate_intra = multi_simulation(name = name, density = intra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 100)\n",
    "    parents_intra, parents_all_intra, points_intra = multi_simulate_intra.simulate_cluster(num_clusters = intra_num_clusters, beta = intra_beta, comp_prob = intra_comp_prob, mean_dist = intra_mean_dist, comp_thr = 1)\n",
    "    \n",
    "    points_CSR_list = []\n",
    "    for i, marker in enumerate(name):\n",
    "        sim = simulation(name = marker, density = CSR_density[i], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 200 + i)\n",
    "        points_CSR_list.append(sim.simulate_CSR())\n",
    "    \n",
    "    points_all = pd.concat([points_extra, points_intra] + points_CSR_list, axis = 0, ignore_index = True)\n",
    "    parents_all = parents_extra\n",
    "    \n",
    "    # ground truth tree and index\n",
    "    tree = make_tree(d1 = np.array(parents_all['global_x']), d2 = np.array(parents_all['global_y']), d3 = np.array(parents_all['global_z']))\n",
    "    ground_truth_indices = set(parents_all.index)\n",
    "    \n",
    "    # run mcDETECT on individual markers/all\n",
    "    for marker in name:\n",
    "        points_marker = points_all[points_all['target'] == marker]\n",
    "        detect_marker = model(shape = shape, transcripts = points_marker, target_all = name, eps = 1.5, in_thr = 0.5, size_thr = 3.5)\n",
    "        sphere_marker = detect_marker.dbscan_single(target_name = marker)\n",
    "        precision, recall, accuracy = metric_main(tree, ground_truth_indices, sphere_marker)\n",
    "\n",
    "        precision_dict[marker].append(precision)\n",
    "        recall_dict[marker].append(recall)\n",
    "        accuracy_dict[marker].append(accuracy)\n",
    "    \n",
    "    detect_all = model(shape = shape, transcripts = points_all, target_all = name, eps = 1.5, in_thr = 0.5, comp_thr = 1, size_thr = 3.5, p = 0.5)\n",
    "    sphere_all = detect_all.merge_data()\n",
    "    precision_all, recall_all, accuracy_all = metric_main(tree, ground_truth_indices, sphere_all)\n",
    "    precision_lst_all.append(precision_all)\n",
    "    recall_lst_all.append(recall_all)\n",
    "    accuracy_lst_all.append(accuracy_all)\n",
    "    \n",
    "    print(f\"{seed} out of {len(seed_lst)} iterations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9112053511367643, 0.9999, 0.9124686572333627)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precision_lst_all), np.mean(recall_lst_all), np.mean(accuracy_lst_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recalls = [np.mean(lst) for lst in recall_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47088, 0.6128600000000001)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(all_recalls), np.max(all_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
