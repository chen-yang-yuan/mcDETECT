{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_recall_curve, auc, f1_score\n",
        "from model import *\n",
        "from simulate import *\n",
        "\n",
        "output_dir = \"output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for saving simulated raw data\n",
        "def write_simulated_data(points_all, out_csv, is_3d):\n",
        "    df = points_all.copy().reset_index(drop = True)\n",
        "    df[\"transcript_id\"] = df.index.astype(int)\n",
        "    out = pd.DataFrame({\"transcript_id\": df[\"transcript_id\"],\n",
        "                        \"x\": df[\"global_x\"].astype(float),\n",
        "                        \"y\": df[\"global_y\"].astype(float),\n",
        "                        \"z\": df[\"global_z\"].astype(float) if is_3d else 0.0,\n",
        "                        \"gene\": df[\"target\"].astype(str)})\n",
        "    os.makedirs(os.path.dirname(out_csv), exist_ok = True)\n",
        "    out.to_csv(out_csv, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for calculating precision, recall, accuracy, F1 score\n",
        "def calculate_metric(ground_truth_indices, matched_index):\n",
        "    \n",
        "    flattened_matches = []\n",
        "    for match in matched_index:\n",
        "        if isinstance(match, tuple):\n",
        "            flattened_matches.extend(match)\n",
        "        elif match != -1:\n",
        "            flattened_matches.append(match)\n",
        "\n",
        "    # 1. True Positives (TP): Unique ground truth points correctly detected\n",
        "    unique_matched_points = set(flattened_matches)\n",
        "    true_positives = len(unique_matched_points & ground_truth_indices)\n",
        "\n",
        "    # 2. False Positives (FP): Detections that didn\"t match any ground truth\n",
        "    false_positives = len([x for x in matched_index if x == -1])\n",
        "\n",
        "    # 3. False Negatives (FN): Ground truth points that were never matched\n",
        "    false_negatives = len(ground_truth_indices - unique_matched_points)\n",
        "\n",
        "    # 4. Total ground truth points (used for recall)\n",
        "    total_ground_truth_points = len(ground_truth_indices)\n",
        "\n",
        "    # 5. Total detections (used for accuracy)\n",
        "    total_detections = len(matched_index)\n",
        "\n",
        "    # 6. Precision\n",
        "    if true_positives + false_positives > 0:\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "    else:\n",
        "        precision = 0.0\n",
        "\n",
        "    # 7. Recall\n",
        "    if true_positives + false_negatives > 0:\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "    else:\n",
        "        recall = 0.0\n",
        "\n",
        "    # 8. Revised Accuracy\n",
        "    true_matches = len([x for x in matched_index if x != -1])  # Count of detections correctly matched\n",
        "    if total_detections > 0:\n",
        "        accuracy = true_matches / total_detections\n",
        "    else:\n",
        "        accuracy = 0.0\n",
        "    \n",
        "    # 9. F1 Score\n",
        "    if precision + recall > 0:\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0.0\n",
        "    \n",
        "    return precision, recall, accuracy, f1\n",
        "\n",
        "# Main metric calculation function\n",
        "def metric_main(tree, ground_truth_indices, sphere):\n",
        "    matched_index = []\n",
        "    for k in range(sphere.shape[0]):\n",
        "        idx = tree.query_ball_point([sphere[\"sphere_x\"].iloc[k], sphere[\"sphere_y\"].iloc[k], sphere[\"sphere_z\"].iloc[k]], sphere[\"sphere_r\"].iloc[k])\n",
        "        if len(idx) == 0:\n",
        "            matched_index.append(-1)\n",
        "        elif len(idx) == 1:\n",
        "            matched_index += idx\n",
        "        elif len(idx) > 1:\n",
        "            matched_index.append(tuple(idx))\n",
        "    return calculate_metric(ground_truth_indices, matched_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize sphere radius distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "point_type = [\"CSR\", \"Extranuclear\", \"Intranuclear\"]\n",
        "ratio = [0.5, 0.25, 0.25]\n",
        "mean_dist_extra = 1\n",
        "mean_dist_intra = 4\n",
        "beta_extra = (1, 19)\n",
        "beta_intra = (19, 1)\n",
        "\n",
        "simulate_z = True\n",
        "name = \"A\"\n",
        "density_overall = 0.16\n",
        "num_clusters_extra = 10000\n",
        "num_clusters_intra = 4000\n",
        "seed = 1\n",
        "\n",
        "for i in range(len(point_type)):\n",
        "    simulate = simulation(name = name, density = density_overall * ratio[i], shape = (2000, 2000), layer_num = 8, layer_gap = 1.5, simulate_z = simulate_z, write_path = output_dir + \"/\", seed = seed)\n",
        "    if i == 0:\n",
        "        points_CSR = simulate.simulate_CSR()\n",
        "        points_CSR[\"type\"] = [point_type[i]] * points_CSR.shape[0]\n",
        "    elif i == 1:\n",
        "        parents_cluster_extra, points_cluster_extra = simulate.simulate_cluster(num_clusters = num_clusters_extra, beta = beta_extra, mean_dist = mean_dist_extra)\n",
        "        points_cluster_extra[\"type\"] = [point_type[i]] * points_cluster_extra.shape[0]\n",
        "    elif i == 2:\n",
        "        parents_cluster_intra, points_cluster_intra = simulate.simulate_cluster(num_clusters = num_clusters_intra, beta = beta_intra, mean_dist = mean_dist_intra)\n",
        "        points_cluster_intra[\"type\"] = [point_type[i]] * points_cluster_intra.shape[0]\n",
        "points_all = pd.concat([points_CSR, points_cluster_extra, points_cluster_intra], axis = 0, ignore_index = True)\n",
        "parents_all = parents_cluster_extra\n",
        "\n",
        "def rg(df):\n",
        "    pts = df[[\"global_x\", \"global_y\", \"global_z\"]].to_numpy()\n",
        "    c = pts.mean(axis=0)\n",
        "    return np.sqrt(((pts - c)**2).sum(axis=1).mean())\n",
        "\n",
        "radii = points_cluster_intra.groupby(\"id\").apply(rg)\n",
        "med = np.nanmean(radii)\n",
        "\n",
        "area = np.pi * (radii) ** 2\n",
        "pd.DataFrame({\"id\": radii.index, \"radius\": radii, \"area\": area}).to_csv(\"output/intranuclear_area.csv\", index = 0)\n",
        "\n",
        "# plt.figure(figsize = (6, 4))\n",
        "# sns.histplot(radii, binwidth=0.2, kde=False, edgecolor=\"gray\")\n",
        "# if np.isfinite(med):\n",
        "#     plt.axvline(med, color=\"red\", linestyle=\"--\", linewidth=1)\n",
        "#     ymax = plt.ylim()[1]\n",
        "#     plt.text(med + 0.5, ymax * 0.95, f\"{med:.2f}\",\n",
        "#             color=\"red\", ha=\"left\", va=\"top\", fontsize=10)\n",
        "# plt.xlabel(\"Aggregate radius\", fontsize=12)\n",
        "# plt.ylabel(\"Frequency\", fontsize=12)\n",
        "# plt.xticks(np.arange(0, 15, 2))\n",
        "# plt.savefig(\"output/cell_radius_hist.png\", dpi=500, bbox_inches=\"tight\")\n",
        "# plt.close()\n",
        "\n",
        "# for points, label in zip([points_cluster_intra, points_cluster_extra], [\"somatic\", \"distal\"]):\n",
        "#     in_soma_ratio = points.groupby(\"id\")[\"in_nucleus\"].mean()\n",
        "#     plt.figure(figsize=(6,4))\n",
        "#     sns.histplot(in_soma_ratio, bins=50, stat=\"density\", edgecolor=\"gray\")\n",
        "#     sns.kdeplot(in_soma_ratio, color=\"red\", linewidth=1)\n",
        "#     plt.xlabel(f\"Mean in-soma ratio per aggregate\", fontsize=12)\n",
        "#     plt.ylabel(\"Frequency\", fontsize=12)\n",
        "#     plt.yticks(np.arange(0, 50, 5))\n",
        "#     plt.savefig(f\"output/mean_in_soma_ratio_hist_{label}.png\", dpi=500, bbox_inches=\"tight\")\n",
        "#     plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single-marker CSR and aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings\n",
        "point_type = [\"CSR\", \"Extranuclear\", \"Intranuclear\"]\n",
        "ratio = [0.5, 0.25, 0.25]\n",
        "mean_dist_extra = 1\n",
        "mean_dist_intra = 4\n",
        "beta_extra = (1, 19)\n",
        "beta_intra = (19, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main simulation loop\n",
        "dimension_settings = {\"3D\": True, \"2D\": False}\n",
        "\n",
        "marker_settings = {\"A\": {\"density\": 0.08, \"num_clusters_extra\": 5000, \"num_clusters_intra\": 2000},\n",
        "            \"B\": {\"density\": 0.04, \"num_clusters_extra\": 3000, \"num_clusters_intra\": 1200},\n",
        "            \"C\": {\"density\": 0.02, \"num_clusters_extra\": 2000, \"num_clusters_intra\": 800}}\n",
        "\n",
        "# marker_settings = {\"D\": {\"density\": 0.01, \"num_clusters_extra\": 1250, \"num_clusters_intra\": 500},\n",
        "#                    \"E\": {\"density\": 0.005, \"num_clusters_extra\": 800, \"num_clusters_intra\": 300},\n",
        "#                    \"F\": {\"density\": 0.0025, \"num_clusters_extra\": 500, \"num_clusters_intra\": 200}}\n",
        "\n",
        "seed_lst = np.arange(1, 201)\n",
        "\n",
        "for dimension, simulate_z in dimension_settings.items():\n",
        "    \n",
        "    print(f\"Running simulations for dimension: {dimension}\")\n",
        "\n",
        "    for name, params in marker_settings.items():\n",
        "        \n",
        "        print(f\"Running simulations for marker: {name}\")\n",
        "        \n",
        "        density_overall = params[\"density\"]\n",
        "        num_clusters_extra = params[\"num_clusters_extra\"]\n",
        "        num_clusters_intra = params[\"num_clusters_intra\"]\n",
        "        \n",
        "        precision_lst = []\n",
        "        recall_lst = []\n",
        "        accuracy_lst = []\n",
        "        f1_lst = []\n",
        "\n",
        "        for seed in seed_lst:\n",
        "\n",
        "            # simulate data\n",
        "            for i in range(len(point_type)):\n",
        "                simulate = simulation(name = name, density = density_overall * ratio[i], shape = (2000, 2000), layer_num = 8, layer_gap = 1.5, simulate_z = simulate_z, write_path = output_dir + \"/\", seed = seed)\n",
        "                if i == 0:\n",
        "                    points_CSR = simulate.simulate_CSR()\n",
        "                    points_CSR[\"type\"] = [point_type[i]] * points_CSR.shape[0]\n",
        "                elif i == 1:\n",
        "                    parents_cluster_extra, points_cluster_extra = simulate.simulate_cluster(num_clusters = num_clusters_extra, beta = beta_extra, mean_dist = mean_dist_extra)\n",
        "                    points_cluster_extra[\"type\"] = [point_type[i]] * points_cluster_extra.shape[0]\n",
        "                elif i == 2:\n",
        "                    parents_cluster_intra, points_cluster_intra = simulate.simulate_cluster(num_clusters = num_clusters_intra, beta = beta_intra, mean_dist = mean_dist_intra)\n",
        "                    points_cluster_intra[\"type\"] = [point_type[i]] * points_cluster_intra.shape[0]\n",
        "            points_all = pd.concat([points_CSR, points_cluster_extra, points_cluster_intra], axis = 0, ignore_index = True)\n",
        "            parents_all = parents_cluster_extra\n",
        "            \n",
        "            # save simulated data\n",
        "            write_simulated_data(points_all, f\"simulated_data/single_marker/{dimension}/{name}/seed_{seed}.csv\", simulate_z)\n",
        "            \n",
        "            # run mcDETECT\n",
        "            detect = model(shape = (2000, 2000), transcripts = points_all, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, size_thr = 4)\n",
        "            # detect = model(shape = (2000, 2000), transcripts = points_all, target_all = [\"D\", \"E\", \"F\"], eps = 1.5, in_thr = 0.25, size_thr = 4)\n",
        "            sphere = detect.dbscan_single(target_name = name)\n",
        "            \n",
        "            # find matched index\n",
        "            tree = make_tree(d1 = np.array(parents_all[\"global_x\"]), d2 = np.array(parents_all[\"global_y\"]), d3 = np.array(parents_all[\"global_z\"]))\n",
        "            ground_truth_indices = set(parents_all.index)\n",
        "            \n",
        "            # calculate all metrics\n",
        "            precision, recall, accuracy, f1 = metric_main(tree, ground_truth_indices, sphere)\n",
        "            precision_lst.append(precision)\n",
        "            recall_lst.append(recall)\n",
        "            accuracy_lst.append(accuracy)\n",
        "            f1_lst.append(f1)\n",
        "            \n",
        "            if seed % 50 == 0:\n",
        "                print(f\"{seed} out of {len(seed_lst)} iterations!\")\n",
        "\n",
        "        results_df = pd.DataFrame({\"Simulation\": seed_lst.tolist(),\n",
        "                                   \"Precision\": precision_lst,\n",
        "                                   \"Recall\": recall_lst, \n",
        "                                   \"Accuracy\": accuracy_lst,\n",
        "                                   \"F1 Score\": f1_lst})\n",
        "        results_df.to_csv(os.path.join(output_dir, f\"single_marker_{dimension}_{name}_{num_clusters_extra}_{num_clusters_intra}.csv\"), index = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-marker CSR and aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings\n",
        "name = [\"A\", \"B\", \"C\"]\n",
        "\n",
        "shape = (2000, 2000)\n",
        "layer_num = 8\n",
        "layer_gap = 1.5\n",
        "write_path = \"\"\n",
        "\n",
        "CSR_density = [0.04, 0.02, 0.01]\n",
        "\n",
        "extra_density = [0.02, 0.01, 0.005]\n",
        "extra_num_clusters = 5000\n",
        "extra_beta = (1, 19)\n",
        "extra_comp_prob = [0.4, 0.3, 0.3]\n",
        "extra_mean_dist = 1\n",
        "\n",
        "intra_density = [0.02, 0.01, 0.005]\n",
        "intra_num_clusters = 1000\n",
        "intra_beta = (19, 1)\n",
        "intra_comp_prob = [0.8, 0.1, 0.1]\n",
        "intra_mean_dist = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main simulation loop\n",
        "dimension_settings = {\"3D\": True, \"2D\": False}\n",
        "seed_lst = np.arange(1, 201)\n",
        "\n",
        "for dimension, simulate_z in dimension_settings.items():\n",
        "    \n",
        "    print(f\"Running multi-marker simulations for dimension: {dimension}\")\n",
        "    \n",
        "    precision_lst_A, recall_lst_A, accuracy_lst_A, f1_lst_A = [], [], [], []\n",
        "    precision_lst_B, recall_lst_B, accuracy_lst_B, f1_lst_B = [], [], [], []\n",
        "    precision_lst_C, recall_lst_C, accuracy_lst_C, f1_lst_C = [], [], [], []\n",
        "    precision_lst_all, recall_lst_all, accuracy_lst_all, f1_lst_all = [], [], [], []\n",
        "\n",
        "    for seed in seed_lst:\n",
        "\n",
        "        # simulate data\n",
        "        multi_simulate_extra = multi_simulation(name = name, density = extra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed)\n",
        "        parents_extra, parents_all_extra, points_extra = multi_simulate_extra.simulate_cluster(num_clusters = extra_num_clusters, beta = extra_beta, comp_prob = extra_comp_prob, mean_dist = extra_mean_dist, comp_thr = 2)\n",
        "        \n",
        "        multi_simulate_intra = multi_simulation(name = name, density = intra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 100)\n",
        "        parents_intra, parents_all_intra, points_intra = multi_simulate_intra.simulate_cluster(num_clusters = intra_num_clusters, beta = intra_beta, comp_prob = intra_comp_prob, mean_dist = intra_mean_dist, comp_thr = 2)\n",
        "        \n",
        "        simulate_A = simulation(name = name[0], density = CSR_density[0], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 200)\n",
        "        points_CSR_A = simulate_A.simulate_CSR()\n",
        "\n",
        "        simulate_B = simulation(name = name[1], density = CSR_density[1], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 300)\n",
        "        points_CSR_B = simulate_B.simulate_CSR()\n",
        "\n",
        "        simulate_C = simulation(name = name[2], density = CSR_density[2], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = simulate_z, write_path = write_path, seed = seed + 400)\n",
        "        points_CSR_C = simulate_C.simulate_CSR()\n",
        "        \n",
        "        parents_all = parents_extra\n",
        "        points_all = pd.concat([points_extra, points_intra, points_CSR_A, points_CSR_B, points_CSR_C], axis = 0, ignore_index = True)\n",
        "        \n",
        "        points_A = points_all[points_all[\"target\"] == \"A\"]\n",
        "        points_B = points_all[points_all[\"target\"] == \"B\"]\n",
        "        points_C = points_all[points_all[\"target\"] == \"C\"]\n",
        "        \n",
        "        # save simulated data\n",
        "        write_simulated_data(points_all, f\"simulated_data/multi_marker/{dimension}/all/seed_{seed}.csv\", simulate_z)\n",
        "        write_simulated_data(points_A, f\"simulated_data/multi_marker/{dimension}/A/seed_{seed}.csv\", simulate_z)\n",
        "        write_simulated_data(points_B, f\"simulated_data/multi_marker/{dimension}/B/seed_{seed}.csv\", simulate_z)\n",
        "        write_simulated_data(points_C, f\"simulated_data/multi_marker/{dimension}/C/seed_{seed}.csv\", simulate_z)\n",
        "        \n",
        "        # ground truth tree and index\n",
        "        tree = make_tree(d1 = np.array(parents_all[\"global_x\"]), d2 = np.array(parents_all[\"global_y\"]), d3 = np.array(parents_all[\"global_z\"]))\n",
        "        ground_truth_indices = set(parents_all.index)\n",
        "        \n",
        "        # run mcDETECT on A/B/C/all\n",
        "        detect_A = model(shape = (2000, 2000), transcripts = points_A, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, size_thr = 4)\n",
        "        sphere_A = detect_A.dbscan_single(target_name = \"A\")\n",
        "        precision_A, recall_A, accuracy_A, f1_A = metric_main(tree, ground_truth_indices, sphere_A)\n",
        "        precision_lst_A.append(precision_A)\n",
        "        recall_lst_A.append(recall_A)\n",
        "        accuracy_lst_A.append(accuracy_A)\n",
        "        f1_lst_A.append(f1_A)\n",
        "        \n",
        "        detect_B = model(shape = (2000, 2000), transcripts = points_B, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, size_thr = 4)\n",
        "        sphere_B = detect_B.dbscan_single(target_name = \"B\")\n",
        "        precision_B, recall_B, accuracy_B, f1_B = metric_main(tree, ground_truth_indices, sphere_B)\n",
        "        precision_lst_B.append(precision_B)\n",
        "        recall_lst_B.append(recall_B)\n",
        "        accuracy_lst_B.append(accuracy_B)\n",
        "        f1_lst_B.append(f1_B)\n",
        "        \n",
        "        detect_C = model(shape = (2000, 2000), transcripts = points_C, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, size_thr = 4)\n",
        "        sphere_C = detect_C.dbscan_single(target_name = \"C\")\n",
        "        precision_C, recall_C, accuracy_C, f1_C = metric_main(tree, ground_truth_indices, sphere_C)\n",
        "        precision_lst_C.append(precision_C)\n",
        "        recall_lst_C.append(recall_C)\n",
        "        accuracy_lst_C.append(accuracy_C)\n",
        "        f1_lst_C.append(f1_C)\n",
        "        \n",
        "        detect_all = model(shape = (2000, 2000), transcripts = points_all, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, comp_thr = 2, size_thr = 4, p = 0.5)\n",
        "        sphere_all = detect_all.merge_data()\n",
        "        precision_all, recall_all, accuracy_all, f1_all = metric_main(tree, ground_truth_indices, sphere_all)\n",
        "        precision_lst_all.append(precision_all)\n",
        "        recall_lst_all.append(recall_all)\n",
        "        accuracy_lst_all.append(accuracy_all)\n",
        "        f1_lst_all.append(f1_all)\n",
        "        \n",
        "        if seed % 50 == 0:\n",
        "            print(\"{} out of {} iterations!\".format(seed, len(seed_lst)))\n",
        "    \n",
        "    pd.DataFrame({\"Simulation\": seed_lst.tolist(), \"Precision\": precision_lst_A, \"Recall\": recall_lst_A, \"Accuracy\": accuracy_lst_A, \"F1\": f1_lst_A}).to_csv(os.path.join(output_dir, f\"multi_marker_{dimension}_A_{extra_num_clusters}_{intra_num_clusters}.csv\"), index = 0)\n",
        "    pd.DataFrame({\"Simulation\": seed_lst.tolist(), \"Precision\": precision_lst_B, \"Recall\": recall_lst_B, \"Accuracy\": accuracy_lst_B, \"F1\": f1_lst_B}).to_csv(os.path.join(output_dir, f\"multi_marker_{dimension}_B_{extra_num_clusters}_{intra_num_clusters}.csv\"), index = 0)\n",
        "    pd.DataFrame({\"Simulation\": seed_lst.tolist(), \"Precision\": precision_lst_C, \"Recall\": recall_lst_C, \"Accuracy\": accuracy_lst_C, \"F1\": f1_lst_C}).to_csv(os.path.join(output_dir, f\"multi_marker_{dimension}_C_{extra_num_clusters}_{intra_num_clusters}.csv\"), index = 0)\n",
        "    pd.DataFrame({\"Simulation\": seed_lst.tolist(), \"Precision\": precision_lst_all, \"Recall\": recall_lst_all, \"Accuracy\": accuracy_lst_all, \"F1\": f1_lst_all}).to_csv(os.path.join(output_dir, f\"multi_marker_{dimension}_all_{extra_num_clusters}_{intra_num_clusters}.csv\"), index = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark parameter p in the multi-marker scenario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up\n",
        "name = [\"A\", \"B\", \"C\"]\n",
        "\n",
        "shape = (2000, 2000)\n",
        "layer_num = 8\n",
        "layer_gap = 1.5\n",
        "write_path = \"\"\n",
        "\n",
        "# CSR_density = [0.04, 0.02, 0.01]\n",
        "CSR_density = [0.02, 0.01, 0.005]\n",
        "\n",
        "# extra_density = [0.02, 0.01, 0.005]\n",
        "extra_density = [0.04, 0.02, 0.01]\n",
        "extra_num_clusters = 2500\n",
        "extra_beta = (1, 19)\n",
        "extra_comp_prob = [0.4, 0.3, 0.3]\n",
        "# extra_mean_dist = 1\n",
        "extra_mean_dist = 0.3\n",
        "\n",
        "intra_density = [0.02, 0.01, 0.005]\n",
        "intra_num_clusters = 1000\n",
        "intra_beta = (19, 1)\n",
        "intra_comp_prob = [0.8, 0.1, 0.1]\n",
        "intra_mean_dist = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark p parameter in multi-marker scenario across multiple seeds\n",
        "p_values = np.arange(0, 1.1, 0.1)\n",
        "\n",
        "# Use multiple seeds for more robust benchmarking\n",
        "benchmark_seeds = np.arange(1, 11)  # 10 seeds\n",
        "\n",
        "# Results storage - will aggregate across seeds\n",
        "num_detections_vs_p = []\n",
        "avg_aggregates_per_transcript_vs_p = []\n",
        "\n",
        "# Storage for per-seed results (for calculating standard errors if needed)\n",
        "all_num_detections = []\n",
        "all_avg_aggregates_per_transcript = []\n",
        "\n",
        "print(\"Benchmarking p parameter across multiple seeds...\")\n",
        "for p in p_values:\n",
        "    # Storage for this p value across all seeds\n",
        "    seed_num_detections = []\n",
        "    seed_avg_aggregates_per_transcript = []\n",
        "    \n",
        "    for benchmark_seed in benchmark_seeds:\n",
        "        \n",
        "        # Simulate data for this seed\n",
        "        multi_simulate_extra = multi_simulation(name = name, density = extra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = True, write_path = write_path, seed = benchmark_seed)\n",
        "        parents_extra, parents_all_extra, points_extra = multi_simulate_extra.simulate_cluster(num_clusters = extra_num_clusters, beta = extra_beta, comp_prob = extra_comp_prob, mean_dist = extra_mean_dist, comp_thr = 2)\n",
        "        \n",
        "        multi_simulate_intra = multi_simulation(name = name, density = intra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = True, write_path = write_path, seed = benchmark_seed + 10)\n",
        "        parents_intra, parents_all_intra, points_intra = multi_simulate_intra.simulate_cluster(num_clusters = intra_num_clusters, beta = intra_beta, comp_prob = intra_comp_prob, mean_dist = intra_mean_dist, comp_thr = 2)\n",
        "        \n",
        "        simulate_A = simulation(name = name[0], density = CSR_density[0], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = True, write_path = write_path, seed = benchmark_seed + 20)\n",
        "        points_CSR_A = simulate_A.simulate_CSR()\n",
        "        \n",
        "        simulate_B = simulation(name = name[1], density = CSR_density[1], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = True, write_path = write_path, seed = benchmark_seed + 30)\n",
        "        points_CSR_B = simulate_B.simulate_CSR()\n",
        "        \n",
        "        simulate_C = simulation(name = name[2], density = CSR_density[2], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = True, write_path = write_path, seed = benchmark_seed + 40)\n",
        "        points_CSR_C = simulate_C.simulate_CSR()\n",
        "        \n",
        "        parents_all = parents_extra\n",
        "        points_all = pd.concat([points_extra, points_intra, points_CSR_A, points_CSR_B, points_CSR_C], axis = 0, ignore_index = True)\n",
        "        \n",
        "        # Run detection with this p value\n",
        "        detect_all = model(shape = (2000, 2000), transcripts = points_all, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.25, comp_thr = 2, size_thr = 4, p = p)\n",
        "        sphere_all = detect_all.merge_data()\n",
        "        \n",
        "        # Count detections\n",
        "        num_detections = sphere_all.shape[0]\n",
        "        seed_num_detections.append(num_detections)\n",
        "        \n",
        "        # Calculate average number of aggregates per transcript\n",
        "        # All detected aggregates are already extrasomatic (filtered by in_thr in the model)\n",
        "        transcript_coords = points_all[[\"global_x\", \"global_y\", \"global_z\"]].values\n",
        "        \n",
        "        # Count how many aggregates each transcript belongs to\n",
        "        transcript_aggregate_count = {}\n",
        "        \n",
        "        for idx, sphere in sphere_all.iterrows():\n",
        "            # Find transcripts within this sphere\n",
        "            center = np.array([sphere[\"sphere_x\"], sphere[\"sphere_y\"], sphere[\"sphere_z\"]])\n",
        "            distances = np.sqrt(((transcript_coords - center) ** 2).sum(axis=1))\n",
        "            within_sphere = distances <= sphere[\"sphere_r\"]\n",
        "            \n",
        "            # Count transcripts in this aggregate\n",
        "            transcript_indices = np.where(within_sphere)[0]\n",
        "            for transcript_idx in transcript_indices:\n",
        "                transcript_aggregate_count[transcript_idx] = transcript_aggregate_count.get(transcript_idx, 0) + 1\n",
        "        \n",
        "        # Calculate average number of aggregates per transcript\n",
        "        if len(transcript_aggregate_count) > 0:\n",
        "            avg_aggregates_per_transcript = np.mean(list(transcript_aggregate_count.values()))\n",
        "        else:\n",
        "            avg_aggregates_per_transcript = 0.0\n",
        "        \n",
        "        seed_avg_aggregates_per_transcript.append(avg_aggregates_per_transcript)\n",
        "    \n",
        "    # Calculate mean across seeds for this p value\n",
        "    num_detections_vs_p.append(np.mean(seed_num_detections))\n",
        "    avg_aggregates_per_transcript_vs_p.append(np.mean(seed_avg_aggregates_per_transcript))\n",
        "    \n",
        "    # Store all seed results for potential analysis\n",
        "    all_num_detections.append(seed_num_detections)\n",
        "    all_avg_aggregates_per_transcript.append(seed_avg_aggregates_per_transcript)\n",
        "    \n",
        "    print(f\"p = {p:.1f}: Mean {np.mean(seed_num_detections):.1f} detections, Mean {np.mean(seed_avg_aggregates_per_transcript):.4f} aggregates per transcript\")\n",
        "\n",
        "# Save mean results\n",
        "p_benchmark_df = pd.DataFrame({\n",
        "    \"p\": p_values,\n",
        "    \"num_detections\": num_detections_vs_p,\n",
        "    \"avg_aggregates_per_transcript\": avg_aggregates_per_transcript_vs_p\n",
        "})\n",
        "p_benchmark_df.to_csv(os.path.join(output_dir, \"p_benchmark_multi_marker_3D_mean.csv\"), index = 0)\n",
        "\n",
        "# Save detailed results with all seeds\n",
        "p_benchmark_df_detailed = pd.DataFrame({\n",
        "    \"p\": np.repeat(p_values, len(benchmark_seeds)),\n",
        "    \"seed\": np.tile(benchmark_seeds, len(p_values)),\n",
        "    \"num_detections\": [item for sublist in all_num_detections for item in sublist],\n",
        "    \"avg_aggregates_per_transcript\": [item for sublist in all_avg_aggregates_per_transcript for item in sublist]\n",
        "})\n",
        "p_benchmark_df_detailed.to_csv(os.path.join(output_dir, \"p_benchmark_multi_marker_3D_detailed.csv\"), index = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Benchmark p parameter in multi-marker scenario across multiple seeds\n",
        "# p_values = np.arange(0, 1.1, 0.1)\n",
        "\n",
        "# # Use multiple seeds for more robust benchmarking\n",
        "# benchmark_seeds = np.arange(1, 11)  # 10 seeds\n",
        "\n",
        "# # Results storage - will aggregate across seeds\n",
        "# num_detections_vs_p = []\n",
        "# accuracy_vs_p = []\n",
        "# precision_vs_p = []\n",
        "# recall_vs_p = []\n",
        "# f1_vs_p = []\n",
        "# auc_pr_vs_p = []\n",
        "\n",
        "# # Storage for per-seed results (for calculating standard errors if needed)\n",
        "# all_num_detections = []\n",
        "# all_accuracy = []\n",
        "# all_precision = []\n",
        "# all_recall = []\n",
        "# all_f1 = []\n",
        "# all_auc_pr = []\n",
        "\n",
        "# print(\"Benchmarking p parameter across multiple seeds...\")\n",
        "# for p in p_values:\n",
        "#     # Storage for this p value across all seeds\n",
        "#     seed_num_detections = []\n",
        "#     seed_accuracy = []\n",
        "#     seed_precision = []\n",
        "#     seed_recall = []\n",
        "#     seed_f1 = []\n",
        "    \n",
        "#     for benchmark_seed in benchmark_seeds:\n",
        "        \n",
        "#         # Simulate data for this seed\n",
        "#         multi_simulate_extra = multi_simulation(name = name, density = extra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = True, write_path = write_path, seed = benchmark_seed)\n",
        "#         parents_extra, parents_all_extra, points_extra = multi_simulate_extra.simulate_cluster(num_clusters = extra_num_clusters, beta = extra_beta, comp_prob = extra_comp_prob, mean_dist = extra_mean_dist, comp_thr = 2)\n",
        "        \n",
        "#         multi_simulate_intra = multi_simulation(name = name, density = intra_density, shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = True, write_path = write_path, seed = benchmark_seed + 100)\n",
        "#         parents_intra, parents_all_intra, points_intra = multi_simulate_intra.simulate_cluster(num_clusters = intra_num_clusters, beta = intra_beta, comp_prob = intra_comp_prob, mean_dist = intra_mean_dist, comp_thr = 2)\n",
        "        \n",
        "#         simulate_A = simulation(name = name[0], density = CSR_density[0], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = True, write_path = write_path, seed = benchmark_seed + 20)\n",
        "#         points_CSR_A = simulate_A.simulate_CSR()\n",
        "        \n",
        "#         simulate_B = simulation(name = name[1], density = CSR_density[1], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = True, write_path = write_path, seed = benchmark_seed + 30)\n",
        "#         points_CSR_B = simulate_B.simulate_CSR()\n",
        "        \n",
        "#         simulate_C = simulation(name = name[2], density = CSR_density[2], shape = shape, layer_num = layer_num, layer_gap = layer_gap, simulate_z = True, write_path = write_path, seed = benchmark_seed + 40)\n",
        "#         points_CSR_C = simulate_C.simulate_CSR()\n",
        "        \n",
        "#         parents_all = parents_extra\n",
        "#         points_all = pd.concat([points_extra, points_intra, points_CSR_A, points_CSR_B, points_CSR_C], axis = 0, ignore_index = True)\n",
        "        \n",
        "#         # Ground truth tree and index\n",
        "#         tree = make_tree(d1 = np.array(parents_all[\"global_x\"]), d2 = np.array(parents_all[\"global_y\"]), d3 = np.array(parents_all[\"global_z\"]))\n",
        "#         ground_truth_indices = set(parents_all.index)\n",
        "        \n",
        "#         # Run detection with this p value\n",
        "#         # Set comp_thr = 2 to require at least 2 marker types (multi-marker filtering)\n",
        "#         detect_all = model(shape = (2000, 2000), transcripts = points_all, target_all = [\"A\", \"B\", \"C\"], eps = 1.5, in_thr = 0.2, comp_thr = 1, size_thr = 3.5, p = p)\n",
        "#         sphere_all = detect_all.merge_data()\n",
        "        \n",
        "#         # Count detections\n",
        "#         num_detections = sphere_all.shape[0]\n",
        "#         seed_num_detections.append(num_detections)\n",
        "        \n",
        "#         # Calculate metrics\n",
        "#         precision, recall, accuracy, f1 = metric_main(tree, ground_truth_indices, sphere_all)\n",
        "#         # For AUC-PR, using precision as proxy (as per the framework)\n",
        "#         auc_pr = precision\n",
        "        \n",
        "#         seed_accuracy.append(accuracy)\n",
        "#         seed_precision.append(precision)\n",
        "#         seed_recall.append(recall)\n",
        "#         seed_f1.append(f1)\n",
        "    \n",
        "#     # Calculate mean across seeds for this p value\n",
        "#     num_detections_vs_p.append(np.mean(seed_num_detections))\n",
        "#     accuracy_vs_p.append(np.mean(seed_accuracy))\n",
        "#     precision_vs_p.append(np.mean(seed_precision))\n",
        "#     recall_vs_p.append(np.mean(seed_recall))\n",
        "#     f1_vs_p.append(np.mean(seed_f1))\n",
        "    \n",
        "#     # Store all seed results for potential analysis\n",
        "#     all_num_detections.append(seed_num_detections)\n",
        "#     all_accuracy.append(seed_accuracy)\n",
        "#     all_precision.append(seed_precision)\n",
        "#     all_recall.append(seed_recall)\n",
        "#     all_f1.append(seed_f1)\n",
        "    \n",
        "#     print(f\"p = {p:.1f}: Mean {np.mean(seed_num_detections):.1f} detections, Precision = {np.mean(seed_precision):.4f}, Recall = {np.mean(seed_recall):.4f}, F1 = {np.mean(seed_f1):.4f}\")\n",
        "\n",
        "# # Save mean results\n",
        "# p_benchmark_df = pd.DataFrame({\n",
        "#     \"p\": p_values,\n",
        "#     \"num_detections\": num_detections_vs_p,\n",
        "#     \"accuracy\": accuracy_vs_p,\n",
        "#     \"precision\": precision_vs_p,\n",
        "#     \"recall\": recall_vs_p,\n",
        "#     \"f1\": f1_vs_p\n",
        "# })\n",
        "# p_benchmark_df.to_csv(os.path.join(output_dir, \"p_benchmark_multi_marker_3D_mean.csv\"), index = 0)\n",
        "\n",
        "# # Save detailed results with standard deviations\n",
        "# p_benchmark_df_detailed = pd.DataFrame({\n",
        "#     \"p\": np.repeat(p_values, len(benchmark_seeds)),\n",
        "#     \"seed\": np.tile(benchmark_seeds, len(p_values)),\n",
        "#     \"num_detections\": [item for sublist in all_num_detections for item in sublist],\n",
        "#     \"accuracy\": [item for sublist in all_accuracy for item in sublist],\n",
        "#     \"precision\": [item for sublist in all_precision for item in sublist],\n",
        "#     \"recall\": [item for sublist in all_recall for item in sublist],\n",
        "#     \"f1\": [item for sublist in all_f1 for item in sublist]\n",
        "# })\n",
        "# p_benchmark_df_detailed.to_csv(os.path.join(output_dir, \"p_benchmark_multi_marker_3D_detailed.csv\"), index = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark ratio between CSR, extranuclear, and intranuclear aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark 1: Vary CSR ratio, keep extra and intra-nuclear ratios identical\n",
        "# Only use A, B, C markers and 3D case\n",
        "\n",
        "# Original marker settings for A, B, C\n",
        "marker_settings_original = {\"A\": {\"density\": 0.08, \"num_clusters_extra\": 5000, \"num_clusters_intra\": 2000},\n",
        "                            \"B\": {\"density\": 0.04, \"num_clusters_extra\": 3000, \"num_clusters_intra\": 1200},\n",
        "                            \"C\": {\"density\": 0.02, \"num_clusters_extra\": 2000, \"num_clusters_intra\": 800}}\n",
        "\n",
        "# Original ratio\n",
        "original_ratio = [0.5, 0.25, 0.25]  # [CSR, Extra, Intra]\n",
        "original_extra_ratio = original_ratio[1]\n",
        "original_intra_ratio = original_ratio[2]\n",
        "\n",
        "# Test 5 different CSR ratios\n",
        "csr_ratios = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "\n",
        "# Settings\n",
        "point_type = [\"CSR\", \"Extranuclear\", \"Intranuclear\"]\n",
        "mean_dist_extra = 1\n",
        "mean_dist_intra = 4\n",
        "beta_extra = (1, 19)\n",
        "beta_intra = (19, 1)\n",
        "simulate_z = True  # 3D only\n",
        "shape_area = 2000 * 2000  # Used for scaling cluster counts\n",
        "\n",
        "seed_lst = np.arange(1, 101)\n",
        "\n",
        "print(\"Benchmark 1: Varying CSR ratio (keeping extra = intra)\")\n",
        "print(f\"Testing CSR ratios: {csr_ratios}\")\n",
        "\n",
        "for csr_ratio in csr_ratios:\n",
        "    # Calculate extra and intra ratios (they must be equal and sum with CSR to 1)\n",
        "    remaining_ratio = 1.0 - csr_ratio\n",
        "    extra_ratio = remaining_ratio / 2.0\n",
        "    intra_ratio = remaining_ratio / 2.0\n",
        "    ratio = [csr_ratio, extra_ratio, intra_ratio]\n",
        "    \n",
        "    print(f\"\\nCSR ratio: {csr_ratio:.2f}, Extra ratio: {extra_ratio:.2f}, Intra ratio: {intra_ratio:.2f}\")\n",
        "    \n",
        "    for name, params_original in marker_settings_original.items():\n",
        "        density_overall = params_original[\"density\"]\n",
        "        \n",
        "        # Calculate scaling factors to maintain approximately same number of transcripts per cluster\n",
        "        # Points per cluster ≈ (density * shape_area) / num_clusters\n",
        "        # To keep points per cluster constant: num_clusters must scale proportionally with density change\n",
        "        # num_clusters_new / num_clusters_original = new_density / original_density = new_ratio / original_ratio\n",
        "        extra_scale_factor = extra_ratio / original_extra_ratio\n",
        "        intra_scale_factor = intra_ratio / original_intra_ratio\n",
        "        \n",
        "        num_clusters_extra = int(params_original[\"num_clusters_extra\"] * extra_scale_factor)\n",
        "        num_clusters_intra = int(params_original[\"num_clusters_intra\"] * intra_scale_factor)\n",
        "        \n",
        "        print(f\"  Marker {name}: num_clusters_extra={num_clusters_extra}, num_clusters_intra={num_clusters_intra}\")\n",
        "        \n",
        "        precision_lst = []\n",
        "        recall_lst = []\n",
        "        accuracy_lst = []\n",
        "        f1_lst = []\n",
        "        \n",
        "        for seed in seed_lst:\n",
        "            # Simulate data\n",
        "            for i in range(len(point_type)):\n",
        "                simulate = simulation(name=name, density=density_overall * ratio[i], shape=(2000, 2000), \n",
        "                                     layer_num=8, layer_gap=1.5, simulate_z=simulate_z, \n",
        "                                     write_path=output_dir + \"/\", seed=seed)\n",
        "                if i == 0:\n",
        "                    points_CSR = simulate.simulate_CSR()\n",
        "                    points_CSR[\"type\"] = [point_type[i]] * points_CSR.shape[0]\n",
        "                elif i == 1:\n",
        "                    parents_cluster_extra, points_cluster_extra = simulate.simulate_cluster(\n",
        "                        num_clusters=num_clusters_extra, beta=beta_extra, mean_dist=mean_dist_extra)\n",
        "                    points_cluster_extra[\"type\"] = [point_type[i]] * points_cluster_extra.shape[0]\n",
        "                elif i == 2:\n",
        "                    parents_cluster_intra, points_cluster_intra = simulate.simulate_cluster(\n",
        "                        num_clusters=num_clusters_intra, beta=beta_intra, mean_dist=mean_dist_intra)\n",
        "                    points_cluster_intra[\"type\"] = [point_type[i]] * points_cluster_intra.shape[0]\n",
        "            \n",
        "            points_all = pd.concat([points_CSR, points_cluster_extra, points_cluster_intra], \n",
        "                                  axis=0, ignore_index=True)\n",
        "            parents_all = parents_cluster_extra\n",
        "            \n",
        "            # Run mcDETECT\n",
        "            detect = model(shape=(2000, 2000), transcripts=points_all, target_all=[\"A\", \"B\", \"C\"], \n",
        "                          eps=1.5, in_thr=0.25, size_thr=4)\n",
        "            sphere = detect.dbscan_single(target_name=name)\n",
        "            \n",
        "            # Find matched index\n",
        "            tree = make_tree(d1=np.array(parents_all[\"global_x\"]), \n",
        "                           d2=np.array(parents_all[\"global_y\"]), \n",
        "                           d3=np.array(parents_all[\"global_z\"]))\n",
        "            ground_truth_indices = set(parents_all.index)\n",
        "            \n",
        "            # Calculate all metrics\n",
        "            precision, recall, accuracy, f1 = metric_main(tree, ground_truth_indices, sphere)\n",
        "            precision_lst.append(precision)\n",
        "            recall_lst.append(recall)\n",
        "            accuracy_lst.append(accuracy)\n",
        "            f1_lst.append(f1)\n",
        "            \n",
        "            if seed % 25 == 0:\n",
        "                print(f\"    {seed} out of {len(seed_lst)} iterations!\")\n",
        "        \n",
        "        # Save results\n",
        "        results_df = pd.DataFrame({\n",
        "            \"Simulation\": seed_lst.tolist(),\n",
        "            \"Precision\": precision_lst,\n",
        "            \"Recall\": recall_lst,\n",
        "            \"Accuracy\": accuracy_lst,\n",
        "            \"F1_Score\": f1_lst,\n",
        "            \"CSR_ratio\": [csr_ratio] * len(seed_lst),\n",
        "            \"Extra_ratio\": [extra_ratio] * len(seed_lst),\n",
        "            \"Intra_ratio\": [intra_ratio] * len(seed_lst)\n",
        "        })\n",
        "        filename = f\"benchmark_ratio_csr_{name}_csr{csr_ratio:.2f}_extra{extra_ratio:.2f}_intra{intra_ratio:.2f}.csv\"\n",
        "        results_df.to_csv(os.path.join(output_dir, filename), index=0)\n",
        "        \n",
        "        print(f\"  Marker {name}: Mean Precision={np.mean(precision_lst):.4f}, \"\n",
        "              f\"Recall={np.mean(recall_lst):.4f}, F1={np.mean(f1_lst):.4f}\")\n",
        "\n",
        "print(\"\\nBenchmark 1 completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark 2: Fix CSR at 0.5, vary the proportion of extra- and intra-nuclear ratios\n",
        "# Only use A, B, C markers and 3D case\n",
        "\n",
        "# Original marker settings for A, B, C\n",
        "marker_settings_original = {\"A\": {\"density\": 0.08, \"num_clusters_extra\": 5000, \"num_clusters_intra\": 2000},\n",
        "                            \"B\": {\"density\": 0.04, \"num_clusters_extra\": 3000, \"num_clusters_intra\": 1200},\n",
        "                            \"C\": {\"density\": 0.02, \"num_clusters_extra\": 2000, \"num_clusters_intra\": 800}}\n",
        "\n",
        "# Original ratio\n",
        "original_ratio = [0.5, 0.25, 0.25]  # [CSR, Extra, Intra]\n",
        "original_extra_ratio = original_ratio[1]\n",
        "original_intra_ratio = original_ratio[2]\n",
        "\n",
        "# Fix CSR at 0.5, test 5 different extra/intra splits\n",
        "# Since CSR = 0.5, extra + intra must sum to 0.5\n",
        "csr_ratio_fixed = 0.5\n",
        "extra_intra_splits = [\n",
        "    (0.4, 0.1),   # Extra-heavy\n",
        "    (0.325, 0.175),\n",
        "    (0.25, 0.25),  # Equal (original)\n",
        "    (0.175, 0.325),\n",
        "    (0.1, 0.4)    # Intra-heavy\n",
        "]\n",
        "\n",
        "# Settings\n",
        "point_type = [\"CSR\", \"Extranuclear\", \"Intranuclear\"]\n",
        "mean_dist_extra = 1\n",
        "mean_dist_intra = 4\n",
        "beta_extra = (1, 19)\n",
        "beta_intra = (19, 1)\n",
        "simulate_z = True  # 3D only\n",
        "shape_area = 2000 * 2000  # Used for scaling cluster counts\n",
        "\n",
        "seed_lst = np.arange(1, 101)\n",
        "\n",
        "print(\"Benchmark 2: Varying extra/intra ratio (fixing CSR at 0.5)\")\n",
        "print(f\"Testing extra/intra splits: {extra_intra_splits}\")\n",
        "\n",
        "for extra_ratio, intra_ratio in extra_intra_splits:\n",
        "    ratio = [csr_ratio_fixed, extra_ratio, intra_ratio]\n",
        "    \n",
        "    print(f\"\\nCSR ratio: {csr_ratio_fixed:.2f}, Extra ratio: {extra_ratio:.2f}, Intra ratio: {intra_ratio:.2f}\")\n",
        "    \n",
        "    for name, params_original in marker_settings_original.items():\n",
        "        density_overall = params_original[\"density\"]\n",
        "        \n",
        "        # Calculate scaling factors to maintain approximately same number of transcripts per cluster\n",
        "        # Points per cluster ≈ (density * shape_area) / num_clusters\n",
        "        # To keep points per cluster constant: num_clusters must scale proportionally with density change\n",
        "        # num_clusters_new / num_clusters_original = new_density / original_density = new_ratio / original_ratio\n",
        "        extra_scale_factor = extra_ratio / original_extra_ratio\n",
        "        intra_scale_factor = intra_ratio / original_intra_ratio\n",
        "        \n",
        "        num_clusters_extra = int(params_original[\"num_clusters_extra\"] * extra_scale_factor)\n",
        "        num_clusters_intra = int(params_original[\"num_clusters_intra\"] * intra_scale_factor)\n",
        "        \n",
        "        print(f\"  Marker {name}: num_clusters_extra={num_clusters_extra}, num_clusters_intra={num_clusters_intra}\")\n",
        "        \n",
        "        precision_lst = []\n",
        "        recall_lst = []\n",
        "        accuracy_lst = []\n",
        "        f1_lst = []\n",
        "        \n",
        "        for seed in seed_lst:\n",
        "            # Simulate data\n",
        "            for i in range(len(point_type)):\n",
        "                simulate = simulation(name=name, density=density_overall * ratio[i], shape=(2000, 2000), \n",
        "                                     layer_num=8, layer_gap=1.5, simulate_z=simulate_z, \n",
        "                                     write_path=output_dir + \"/\", seed=seed)\n",
        "                if i == 0:\n",
        "                    points_CSR = simulate.simulate_CSR()\n",
        "                    points_CSR[\"type\"] = [point_type[i]] * points_CSR.shape[0]\n",
        "                elif i == 1:\n",
        "                    parents_cluster_extra, points_cluster_extra = simulate.simulate_cluster(\n",
        "                        num_clusters=num_clusters_extra, beta=beta_extra, mean_dist=mean_dist_extra)\n",
        "                    points_cluster_extra[\"type\"] = [point_type[i]] * points_cluster_extra.shape[0]\n",
        "                elif i == 2:\n",
        "                    parents_cluster_intra, points_cluster_intra = simulate.simulate_cluster(\n",
        "                        num_clusters=num_clusters_intra, beta=beta_intra, mean_dist=mean_dist_intra)\n",
        "                    points_cluster_intra[\"type\"] = [point_type[i]] * points_cluster_intra.shape[0]\n",
        "            \n",
        "            points_all = pd.concat([points_CSR, points_cluster_extra, points_cluster_intra], \n",
        "                                  axis=0, ignore_index=True)\n",
        "            parents_all = parents_cluster_extra\n",
        "            \n",
        "            # Run mcDETECT\n",
        "            detect = model(shape=(2000, 2000), transcripts=points_all, target_all=[\"A\", \"B\", \"C\"], \n",
        "                          eps=1.5, in_thr=0.25, size_thr=4)\n",
        "            sphere = detect.dbscan_single(target_name=name)\n",
        "            \n",
        "            # Find matched index\n",
        "            tree = make_tree(d1=np.array(parents_all[\"global_x\"]), \n",
        "                           d2=np.array(parents_all[\"global_y\"]), \n",
        "                           d3=np.array(parents_all[\"global_z\"]))\n",
        "            ground_truth_indices = set(parents_all.index)\n",
        "            \n",
        "            # Calculate all metrics\n",
        "            precision, recall, accuracy, f1 = metric_main(tree, ground_truth_indices, sphere)\n",
        "            precision_lst.append(precision)\n",
        "            recall_lst.append(recall)\n",
        "            accuracy_lst.append(accuracy)\n",
        "            f1_lst.append(f1)\n",
        "            \n",
        "            if seed % 25 == 0:\n",
        "                print(f\"    {seed} out of {len(seed_lst)} iterations!\")\n",
        "        \n",
        "        # Save results\n",
        "        results_df = pd.DataFrame({\n",
        "            \"Simulation\": seed_lst.tolist(),\n",
        "            \"Precision\": precision_lst,\n",
        "            \"Recall\": recall_lst,\n",
        "            \"Accuracy\": accuracy_lst,\n",
        "            \"F1_Score\": f1_lst,\n",
        "            \"CSR_ratio\": [csr_ratio_fixed] * len(seed_lst),\n",
        "            \"Extra_ratio\": [extra_ratio] * len(seed_lst),\n",
        "            \"Intra_ratio\": [intra_ratio] * len(seed_lst)\n",
        "        })\n",
        "        filename = f\"benchmark_ratio_fixedcsr_{name}_csr{csr_ratio_fixed:.2f}_extra{extra_ratio:.3f}_intra{intra_ratio:.3f}.csv\"\n",
        "        results_df.to_csv(os.path.join(output_dir, filename), index=0)\n",
        "        \n",
        "        print(f\"  Marker {name}: Mean Precision={np.mean(precision_lst):.4f}, \"\n",
        "              f\"Recall={np.mean(recall_lst):.4f}, F1={np.mean(f1_lst):.4f}\")\n",
        "\n",
        "print(\"\\nBenchmark 2 completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mcDETECT-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
